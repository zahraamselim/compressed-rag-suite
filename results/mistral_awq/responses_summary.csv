id,question,ground_truth,rag_answer,no_rag_answer,chunks,avg_score,rag_words,no_rag_words,retrieval_ms,rag_gen_ms,no_rag_gen_ms
1,What is the main focus of this survey paper?,This paper presents a comprehensive survey of model compression techniques for Large Language Models...,The main focus of this survey paper is to provide an overview of the current landscape of LLM compre...,,3,0.6415100807187759,24,0,24.612298999272753,6674.043097999856,3009.1726030004793
2,What challenges do Large Language Models face?,LLMs face challenges stemming from their extensive size and computational requirements. For instance...,,"Data quality and quantity: LLMs require vast amounts of high-quality data to train effectively, but ...",3,0.8353432095659379,0,69,24.380551999456657,8234.712238000611,9991.63646099987
3,What is model compression and why is it important?,"Model compression involves transforming a large, resource-intensive model into a compact version sui...",Model compression refers to the process of reducing the size of large language models (LLMs) while m...,"Model compression refers to the process of reducing the size, complexity or accuracy of a machine le...",3,0.8734372140087743,84,68,25.252458000068145,11315.606244999799,8354.889907999677
4,What are the main categories of model compression techniques covered in this survey?,"The survey covers four main categories: (1) Quantization, (2) Pruning, (3) Knowledge Distillation, a...",,,3,0.898375500430778,0,0,39.667347999966296,6924.071888999606,2194.0089899999293
5,What metrics are used to evaluate compressed LLMs?,"Key metrics include: Model Size (number of parameters), FLOPs (floating-point operations), MFU (Mean...",,Perplexity (PPL) - measures how well a model predicts the next word in a sentence. Lower PPL indicat...,3,0.8606086121098802,0,46,25.844707999567618,7814.779256999827,10016.641789999994
6,What is the difference between FLOPs and MFU?,FLOPs measures the theoretical computational efficiency (number of floating-point operations require...,"FLOPs and MFU are both metrics used to evaluate the efficiency of compressed LLMs. However, they ser...",FLOPS (floating-point operations per second) and MFU (most frequently used) are two different concep...,3,0.6760161045193295,46,72,25.080978999540093,9704.39254899975,9988.710381000601
7,What is quantization in the context of LLMs?,"Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameter...",Quantization in the context of LLMs refers to the process of reducing the precision of model paramet...,"Quantization refers to the process of reducing the precision of numerical values used by a model, su...",3,0.8512720921970711,55,31,25.564855999618885,10226.42653999992,4227.252306000082
8,What is the difference between QAT and PTQ?,The primary distinction lies in whether retraining is needed during quantization. Post-Training Quan...,,"QAT stands for Quality Assurance Testing, while PTQ stands for Performance Testing. Both are types o...",3,0.6875701087998861,0,59,25.752320999345102,10677.87921099989,10067.37779300056
9,What are the three categories of Post-Training Quantization?,PTQ for LLMs is categorized into three groups: (1) Weight-Only Quantization - focuses solely on quan...,The three categories of Post-Training Quantization are: 1. Direct Use of Quantized Models in Inferen...,Model Compression: This category involves reducing the size of a trained model by removing unnecessa...,3,0.7957004113376106,22,83,26.679364999836253,7345.214093999857,9991.506689999369
10,What is GPTQ and how does it work?,GPTQ is a layer-wise quantization method based on Optimal Brain Quantization (OBQ) that updates weig...,GPTQ stands for Gradient-based Pruning and Quantization. It is a layer-wise quantization method prop...,GPTQ (Generative Pre-Training for Querying) is a technique used to improve the performance of query ...,3,0.7264488477579647,38,83,25.274593999711215,8585.914528999638,10093.008427999848
11,What is AWQ's approach to weight quantization?,AWQ (Activation-aware Weight Quantization) stores the top 1% of weights that have the most significa...,,"AWQ uses a combination of techniques such as pruning, quantization, and compression to reduce the si...",3,0.8700377680182824,0,42,25.59050199943158,10452.23812399945,6414.675877999798
12,Why do LLMs have issues with activation quantization?,"LLMs have outliers in activations, and the performance declines significantly if these activations w...",LLMs have issues with activation quantization because it can lead to significant loss of accuracy du...,Activation quantization is a technique used to reduce the precision of activations in neural network...,3,0.8708407632855657,74,93,24.971562000246195,10578.954997999972,9981.290730000183
13,What is SmoothQuant's approach?,SmoothQuant designs a per-channel scaling transformation to smooth the activation outliers based on ...,SmoothQuant's approach involves using smooth quantization functions to gradually decrease the precis...,SmoothQuant uses a combination of statistical and machine learning techniques to analyze financial d...,3,0.6028492077229034,17,61,25.102406000769406,9633.99019299959,8399.485881000146
14,What is KV cache quantization and why is it important?,"KV cache quantization targets the KV cache, which stores keys and values of attention layers. It's i...",KV cache quantization refers to the process of reducing the memory footprint of large language model...,KV Cache Quantization is a technique used to reduce the memory footprint of key-value (KV) pairs in ...,3,0.8364306615053358,62,79,25.20297999944887,9575.318991999666,9958.266376999745
15,What is pruning in neural networks?,Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant co...,Pruning in neural networks refers to the process of removing unnecessary or redundant connections fr...,Pruning in neural networks refers to the process of removing unnecessary or redundant connections be...,3,0.8764424085163723,92,37,25.680699999611534,22778.716652000185,6726.435117000619
16,What are the three types of pruning methods?,The three types are: (1) Unstructured Pruning - prunes individual parameters resulting in irregular ...,Magnitude-based Pruning 2. Regularization-based Pruning 3.,The three main types of pruning methods are: 1. Pre-pruning: This method involves removing unwanted ...,3,0.8293721327649072,6,65,25.12398100043356,7574.74776699928,9958.52633100003
17,What is SparseGPT?,SparseGPT is a one-shot pruning strategy without retraining that frames pruning as an extensive spar...,SparseGPT is an innovative approach that uses a one-shot pruning strategy without retraining to achi...,SparseGPT is a variant of the Generative Adversarial Network (GAN) architecture that uses sparse cod...,3,0.8219295003715263,33,66,25.74377299970365,8469.63523300019,9537.843036000595
18,What is Wanda's pruning approach?,Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm...,,"Wanda uses a pruning approach called ""prune-and-retrain"" to reduce the size of her model. This invol...",3,0.8266254676775334,0,35,25.439484999878914,9349.04064500006,5573.370920000343
19,What are the three categories of structured pruning metrics?,Structured pruning works are divided into: (1) Loss-based Pruning - assesses significance by measuri...,Structured fluctuation metric used by FLAP. Principal Component Analysis (PCA) used by SliceGPT.,The three categories of structured pruning metrics are: 1. Sparsity-based metrics: These metrics mea...,3,0.8444777721853587,13,56,26.13159700013057,15591.107657000066,10002.26165499953
20,What is N:M sparsity?,N:M sparsity is a semi-structured pruning pattern where every M contiguous elements leave N non-zero...,N:M sparsity refers to a technique used in neural network pruning where the weight matrix is divided...,,3,0.7817364308621997,44,0,25.087119000090752,8370.49524000031,396.71314600036567
