id,question,ground_truth,rag_answer,no_rag_answer,chunks,avg_score,rag_words,no_rag_words,retrieval_ms,rag_gen_ms,no_rag_gen_ms
1,What is the main focus of this survey paper?,This paper presents a comprehensive survey of model compression techniques for Large Language Models...,The main focus of this survey paper is to provide insights into the current landscape of LLM compres...,"Without a specific survey paper in mind, I am unable to determine its main focus.",3,0.6415100807187759,26,15,30.773275000683498,4828.576902999885,2384.0009069999724
2,What challenges do Large Language Models face?,LLMs face challenges stemming from their extensive size and computational requirements. For instance...,,Large language models (LLMs) are a type of artificial intelligence that can generate human-like text...,3,0.8353432095659379,0,71,23.821556999791937,5018.322448000617,8163.091272000202
3,What is model compression and why is it important?,"Model compression involves transforming a large, resource-intensive model into a compact version sui...",Model compression refers to the process of reducing the size of large language models (LLMs) while m...,"Model compression refers to the process of reducing the size, complexity or both of a machine learni...",3,0.8734372140087743,49,89,23.750411999571952,5646.0468639998,8168.527035999432
4,What are the main categories of model compression techniques covered in this survey?,"The survey covers four main categories: (1) Quantization, (2) Pruning, (3) Knowledge Distillation, a...",,The main categories of model compression techniques covered in this survey include: 1. Pruning and q...,3,0.898375500430778,0,90,30.189905999577604,4012.8658889998405,8153.055122999831
5,What metrics are used to evaluate compressed LLMs?,"Key metrics include: Model Size (number of parameters), FLOPs (floating-point operations), MFU (Mean...",,Compressed language models (LLMs) can be evaluated using various metrics depending on the specific t...,3,0.8606086121098802,0,53,23.86815000045317,10654.787467999995,8134.547700999974
6,What is the difference between FLOPs and MFU?,FLOPs measures the theoretical computational efficiency (number of floating-point operations require...,"FLOPs and MFU are both metrics used to evaluate the efficiency of machine learning models, but they ...",FLOPs (floating-point operations per second) and MFU (most frequently used) are two different concep...,3,0.6760161045193295,77,87,24.89200800027902,10631.588811000256,8141.530095000235
7,What is quantization in the context of LLMs?,"Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameter...","In the context of LLMs, quantization refers to the process of reducing the precision of model parame...","Quantization refers to the process of reducing the precision or accuracy of a model's parameters, we...",3,0.8512720921970711,58,63,24.389029999838385,8578.42790199993,8143.988036999872
8,What is the difference between QAT and PTQ?,The primary distinction lies in whether retraining is needed during quantization. Post-Training Quan...,,"QAT stands for Quality Assurance Testing, which is a process of testing software or systems to ensur...",3,0.6875701087998861,0,66,23.060868000357004,8453.492417999769,8147.043945000405
9,What are the three categories of Post-Training Quantization?,PTQ for LLMs is categorized into three groups: (1) Weight-Only Quantization - focuses solely on quan...,The three categories of Post-Training Quantization are: 1. Direct Use of Quantized Models in Inferen...,"There are several methods for quantizing models after training, but three common categories include:...",3,0.7957004113376106,21,79,25.14668999992864,3829.284073999588,8157.569713000157
10,What is GPTQ and how does it work?,GPTQ is a layer-wise quantization method based on Optimal Brain Quantization (OBQ) that updates weig...,GPTQ stands for Gradient-based Pruning and Quantization. It is a layer-wise quantization method base...,"GPTQ stands for ""Generative Pre-Training of Question Answering"" and is a type of natural language pr...",3,0.7264488477579647,19,89,24.21260099981737,4320.81102400025,8145.458561999476
11,What is AWQ's approach to weight quantization?,AWQ (Activation-aware Weight Quantization) stores the top 1% of weights that have the most significa...,"AWQ's approach to weight quantization involves identifying sensitive weights, which are those that h...",AWQ (Adaptive Weight Quantization) is a technique used in deep learning models for reducing the prec...,3,0.8700377680182824,29,91,24.806168999930378,4711.107824999999,8147.086599999966
12,Why do LLMs have issues with activation quantization?,"LLMs have outliers in activations, and the performance declines significantly if these activations w...",LLMs have issues with activation quantization because it can lead to a decrease in their accuracy du...,Activation quantization is a technique used to reduce the precision of activations in neural network...,3,0.8708407632855657,70,72,24.387450000176614,8160.492689000421,8133.990516000267
13,What is SmoothQuant's approach?,SmoothQuant designs a per-channel scaling transformation to smooth the activation outliers based on ...,"SmoothQuant's approach involves using a combination of methods such as layer normalization, linear l...",SmoothQuant is a quantitative trading platform that uses machine learning algorithms to analyze mark...,3,0.6028492077229034,50,91,23.91777600041678,13576.374947000659,8138.09651199972
14,What is KV cache quantization and why is it important?,"KV cache quantization targets the KV cache, which stores keys and values of attention layers. It's i...",KV cache quantization refers to the process of reducing the memory footprint of knowledge vectors (K...,Key-Value (KV) cache quantization refers to the process of reducing the precision or accuracy of val...,3,0.8364306615053358,60,73,23.103994999473798,15406.880338999144,8138.045925999904
15,What is pruning in neural networks?,Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant co...,Pruning in neural networks refers to the process of removing unnecessary or redundant connections be...,Pruning in neural networks refers to the process of removing unnecessary or redundant connections be...,3,0.8764424085163723,97,73,23.538618999737082,8686.469975999898,8142.165651000141
16,What are the three types of pruning methods?,The three types are: (1) Unstructured Pruning - prunes individual parameters resulting in irregular ...,Magnitude-based Pruning 2. Regularization-based Pruning 3.,Pruning is a technique used to improve the accuracy and performance of machine learning models by re...,3,0.8293721327649072,6,67,24.114855999869178,10931.911101000878,8141.355210999791
17,What is SparseGPT?,SparseGPT is a one-shot pruning strategy without retraining that frames pruning as an extensive spar...,SparseGPT is an innovative approach in the domain of unstructured pruning of LLMs that introduces a ...,SparseGPT is a variant of the language model GPT (Generative Pre-Trained Transformer) that has been ...,3,0.8219295003715263,39,74,24.702880999939225,15264.593101999708,8113.366423999651
18,What is Wanda's pruning approach?,Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm...,,"Wanda, a large language model trained by Mistral AI, uses a technique called ""pruning"" to reduce the...",3,0.8266254676775334,0,80,23.94843199999741,6000.380794999728,8134.759016999851
19,What are the three categories of structured pruning metrics?,Structured pruning works are divided into: (1) Loss-based Pruning - assesses significance by measuri...,Structured fluctuation metric used by FLAP to assess and identify columns in the weight matrix suita...,Structured pruning metrics can be categorized into three main groups: 1. **Pruning Efficiency**: Thi...,3,0.8444777721853587,30,90,23.872634999861475,14584.488245999637,8142.438668000068
20,What is N:M sparsity?,N:M sparsity is a semi-structured pruning pattern where every M contiguous elements leave N non-zero...,N:M sparsity refers to a method of pruning neural network models where the weight matrix is divided ...,"In the context of matrices, ""N:M"" refers to a specific type of sparsity pattern. Sparsity in a matri...",3,0.7817364308621997,44,55,23.58960899982776,5412.354177000452,8134.495066999989
