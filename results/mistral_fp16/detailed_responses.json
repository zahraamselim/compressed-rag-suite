[
  {
    "question_id": 1,
    "question": "What is the main focus of this survey paper?",
    "ground_truth": "This paper presents a comprehensive survey of model compression techniques for Large Language Models (LLMs), covering methods like quantization, pruning, knowledge distillation, and low-rank factorization.",
    "rag_answer": "The main focus of this survey paper is to provide insights into the current landscape of LLM compression and promote ongoing exploration of this pivotal topic.",
    "no_rag_answer": "Without a specific survey paper in mind, I am unable to determine its main focus.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.6852592213434647,
      0.6435099317621802,
      0.5957610890506828
    ],
    "avg_context_score": 0.6415100807187759,
    "full_context": "methods, metrics, and benchmark datasets. By diving into LLM compression, we've highlighted its challenges and opportunities. This survey aims to be a valuable reference, providing insights into the current landscape and promoting ongoing exploration of this pivotal topic. Acknowledgments We would like to thank the anonymous reviewers and the Action Editor for their valuable feedback and discussions. The work of Jian Li is supported partially by National Natural Science Foundation of China . The work of Yong Liu is supported partially by National Natural Science Foundation of China , Beijing Outstanding Young Scientist Program , the Unicom Innovation Ecological Cooperation Plan, and the CCF-Huawei Populus Grove Fund.\n\nBenchmarks and Datasets The main goal of these benchmarks and datasets is to measure the ef ficiency and performance of compressed LLMs in comparison to their uncompressed counterparts. These benchmarks and datasets typically consist of diverse tasks and datasets that cover a range of natural language processing challenges. Common Benchmarks and Datasets The majority of research evaluates compressed LLMs on well-established NLP benchmarks and datasets. For instance, WikiText-2, C4, and PTB are designed for evaluating the perplexity performance of language models. LAMBADA, PIQA, and OpenBookQA are designed to evaluate the zero-shot ability of language models. GSM8K, CommonsenseQA and StrategyQA are designed to evaluate the reasoning ability of language models. BIG-Bench BIG-Bench (BBH) is a benchmark suite designed for LLMs, covering over 200 NLP tasks, e.g., Text Comprehension Tasks, Inference Tasks, Mathematical Reasoning Tasks. The aim of BBH is to evaluate the performance of LLMs across these various complex tasks. The compressed LLMs use BBH to measure their capability across a multidimensional spectrum of tasks. Unseen Instructions Datasets Unseen instructions datasets are used to evaluate the performance of LLMs on unseen tasks. For instance, the Vicuna-Instructions dataset created by GPT -4 includes 80 complex questions across nine dif ferent categories like generic, knowledge-based, and writing tasks. Another dataset, User -Oriented-Instructions, consists of 252 carefully selected instructions inspired by various user -focused applications such as Grammarly , StackOverflow , and Overleaf. These datasets evaluate how well compact LLMs can handle and carry out new tasks by presenting them with unfamiliar instructions. EleutherAI LM Harness The EleutherAI LM Harness is an advanced framework for evaluating LLMs, providing a unified testing platform that supports over 60 standard academic benchmarks along with hundreds of subtasks and variants. The standardized evaluation tasks provided by the harness ensure the reproducibility and comparability of evaluation, which is essential for implementing fair and reproducible evaluations for the compressed LLMs. Quantization Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameters of the model with minimal loss in inference performance. Quantization can be categorized into two main approaches: Quantization-A ware Training (QA T), and Post-T raining Quantization (PTQ). The primary distinction between\n\nteacher forcing on CoT s within the context, jointly optimizing the likelihood of all in-context CoT s, thereby distilling the capabilities of in-context learning and reasoning into smaller models. Instruction Following Distillation IF aims to bolster the zero-shot ability of LLMs through fine-tuning using a collection of instruction-like prompt-response pairs. For instance, Lion prompts the LLM to identify and generate the \"hard\" instructions, which are then utilized to enhance the student model's capabilities. LaMini-LM develops an extensive collection of 2.58 million instructions, comprising both existing and newly generated instructions, and fine-tunes a diverse array of models by using these instructions. SELF-INSTRUCT uses student LMs themselves as teachers to generate instruction following dataset, and fine-tunes students themselves with the dataset. Selective Reflection- Tuning leverages the teacher LLMs to reflect on and improve existing data, while the student LMs assess and selectively incorporate these improvements, thereby increasing data quality and compatibility with the student LMs. Remark: Black-Box Distillation uses the teacher model's outputs as supervision, but the teacher model's outputs may not cover all possible input scenarios. Thus, understanding how to handle a student model's generalization on unknown data and how to increase data diversity is an area that requires further investigation. White-box KD White-box KD enables the student LM to gain a deeper understanding of the teacher LLM's internal structure and knowledge representations, often resulting in higher -level performance improvements. An representative example is MINILLM, which the first work to study distillation from the Open-source generative LLMs. MINILLM use a reverse Kullback-Leibler diver gence objective, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution, and derives an ef fective optimization approach to learn the objective. Further , GKD explores distillation from auto-regressive models, where generative language models are a subset. GKD trains the student using self-generated outputs, incorporating teacher feedback, and allows flexibility in using dif ferent loss functions when the student cannot fully replicate the teacher's distribution. Dif ferent from the above works, which focus on learning the teacher distribution, TED proposes a task-aware layer -wise distillation method, which designs task-aware filters, which align the hidden representations of the teacher and student models at each intermediate layer , to reduce the knowledge gap between the student and teacher models. Remark: Although white-box distillation allows student LMs to learn the knowledge of teacher LLMs more deeply compared to black-box distillation, currently , open-source LLMs perform worse than closed-source ones, limiting the improvement of student LMs performance in white-box distillation. This is one of the barren factors hindering the development of white-box distillation. A feasible solution is to distill knowledge from closed-source LLMs to open-source LLMs through black-box distillation, and then use white-box distillation to transfer knowledge from open-source LLMs to student LLMs.",
    "context_length_chars": 6595,
    "rag_answer_length_words": 26,
    "no_rag_answer_length_words": 15,
    "retrieval_time_ms": 30.773275000683498,
    "rag_generation_time_ms": 4828.576902999885,
    "no_rag_generation_time_ms": 2384.0009069999724,
    "total_rag_time_ms": 4859.350178000568
  },
  {
    "question_id": 2,
    "question": "What challenges do Large Language Models face?",
    "ground_truth": "LLMs face challenges stemming from their extensive size and computational requirements. For instance, GPT-175B with 175 billion parameters demands a minimum of 350GB of memory in half-precision (FP16) format and requires at least five A100 GPUs with 80GB of memory each for efficient inference.",
    "rag_answer": "",
    "no_rag_answer": "Large language models (LLMs) are a type of artificial intelligence that can generate human-like text based on input prompts. While LLMs have made significant progress in recent years, they still face several challenges: 1. Data quality and quantity: LLMs rely heavily on large amounts of high-quality data to learn patterns and generate coherent text. However, the quality and quantity of available data can be limited, especially for certain languages or topics.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.884886263142107,
      0.8863503632522747,
      0.7347930023034319
    ],
    "avg_context_score": 0.8353432095659379,
    "full_context": "A Survey on Model Compr ession for Large Language Models Authors: Xunyu Zhu, Jian Li (Corresponding author), Yong Liu, Can Ma, Weiping Wang Affiliations: Institute of Information Engineering, Chinese Academy of Sciences School of Cyber Security , University of Chinese Academy of Sciences Gaoling School of Artificial Intelligence, Renmin University of China Contact: zhuxunyu@iie.ac.cn , lijian9026@iie.ac.cn , macan@iie.ac.cn , wangweiping@iie.ac.cn , liuyonggsai@ruc.edu.cn Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully . Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emer ged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey of fers valuable insights for researchers and practitioners, aiming to enhance ef ficiency and real-world applicability of LLMs while laying a foundation for future advancements. Introduction Large Language Models (LLMs) refer to Transformer language models that contain billions (or more) of parameters, which are trained on massive text data. LLMs consistently exhibit remarkable performance across various tasks, but their exceptional capabilities come with significant challenges stemming from their extensive size and computational requirements. For instance, the GPT -175B model, with an impressive 175 billion parameters, demands a minimum of 350GB of memory in half-precision (FP16) format. Furthermore, deploying this model for inference necessitates at least five A100 GPUs, each featuring 80GB of memory , to ef ficiently manage operations. To tackle these issues, a prevalent approach known as model compression of fers a solution. Model compression involves transforming a lar ge, resource-intensive model into a compact version suitable for deployment on resource-constrained devices. Additionally , model compression can enhance LLM inference speed and optimizes resource ef ficiency . In our paper , our primary objective is to illuminate the recent strides made in the domain of model compression techniques tailored specifically for LLMs. Our work conducts an exhaustive survey of methodologies, metrics, and benchmarks of model compression for LLMs. The taxonomy of model compression methods for LLMs\n\nBenchmarks and Datasets The main goal of these benchmarks and datasets is to measure the ef ficiency and performance of compressed LLMs in comparison to their uncompressed counterparts. These benchmarks and datasets typically consist of diverse tasks and datasets that cover a range of natural language processing challenges. Common Benchmarks and Datasets The majority of research evaluates compressed LLMs on well-established NLP benchmarks and datasets. For instance, WikiText-2, C4, and PTB are designed for evaluating the perplexity performance of language models. LAMBADA, PIQA, and OpenBookQA are designed to evaluate the zero-shot ability of language models. GSM8K, CommonsenseQA and StrategyQA are designed to evaluate the reasoning ability of language models. BIG-Bench BIG-Bench (BBH) is a benchmark suite designed for LLMs, covering over 200 NLP tasks, e.g., Text Comprehension Tasks, Inference Tasks, Mathematical Reasoning Tasks. The aim of BBH is to evaluate the performance of LLMs across these various complex tasks. The compressed LLMs use BBH to measure their capability across a multidimensional spectrum of tasks. Unseen Instructions Datasets Unseen instructions datasets are used to evaluate the performance of LLMs on unseen tasks. For instance, the Vicuna-Instructions dataset created by GPT -4 includes 80 complex questions across nine dif ferent categories like generic, knowledge-based, and writing tasks. Another dataset, User -Oriented-Instructions, consists of 252 carefully selected instructions inspired by various user -focused applications such as Grammarly , StackOverflow , and Overleaf. These datasets evaluate how well compact LLMs can handle and carry out new tasks by presenting them with unfamiliar instructions. EleutherAI LM Harness The EleutherAI LM Harness is an advanced framework for evaluating LLMs, providing a unified testing platform that supports over 60 standard academic benchmarks along with hundreds of subtasks and variants. The standardized evaluation tasks provided by the harness ensure the reproducibility and comparability of evaluation, which is essential for implementing fair and reproducible evaluations for the compressed LLMs. Quantization Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameters of the model with minimal loss in inference performance. Quantization can be categorized into two main approaches: Quantization-A ware Training (QA T), and Post-T raining Quantization (PTQ). The primary distinction between\n\nLLM Infer ence and Deployment The ef ficiency of compressed LLMs during deployment is also a significant area for exploration. This involves multiple evaluation metrics, including arithmetic intensity , memory size, and throughput. Furthermore, we can use an analytical tool, the Roofline Model, to assess the resource ef ficiency of compressed LLMs on specific hardware. Evaluating the deployment ef ficiency of compressed LLMs on specific hardware can guide researchers in selecting and analyzing the advantages and disadvantages of various model compression methods and further optimizing these methods. The Effect of Scaling Law The scaling law underscores the significant impact of model size, dataset size, and compute resources on the performance of LLMs. However , the scaling law presents a fundamental challenge for LLM compression, i.e., there is a trade-of f between model size and performance in compressed LLMs. Delving into the mechanisms and theories underpinning the scaling law is crucial for elucidating and potentially overcoming this limitation. AutoML for LLM Compr ession Existing compression techniques have made remarkable progress, but they still heavily depend on manual design. For instance, designing appropriate student architectures for knowledge distillation requires a significant amount of human ef fort. To reduce this reliance on manual design, a feasible solution is to combine Automated Machine Learning (AutoML) techniques such as Meta-Learning and Neural Architecture Search (NAS) with model compression. By combining with AutoML techniques, model compression can automatically select appropriate hyperparameters and tailor architectures and scales of compressed models, thus minimizing human involvement and lowering the associated costs. Furthermore, AutoML can identify optimal model compression strategies tailored to specific task requirements, thereby further enhancing compression rates without compromising model performance. Explainability of LLM Compr ession Earlier research has raised significant concerns regarding the explainability of model compression techniques applied to Pre-trained Language Models (PLMs). Notably , these same challenges extend to LLM compression methods as well. For example, CoT -distillation can enhance SLMs' reasoning performance, yet the mechanism through which it imparts CoT ability remains unclear . This challenge underscores the importance of integrating explainability with model compression approaches for the advancement of LLM compression applications. Explainability not only clarifies the changes and trade-of fs in the compression process but also enhances efficiency and accuracy . Additionally , interpretability aids in evaluating the compressed model's performance to ensure it aligns with practical requirements. Conclusion In the survey , we have explored model compression techniques for LLMs. Our coverage spanned compression",
    "context_length_chars": 8066,
    "rag_answer_length_words": 0,
    "no_rag_answer_length_words": 71,
    "retrieval_time_ms": 23.821556999791937,
    "rag_generation_time_ms": 5018.322448000617,
    "no_rag_generation_time_ms": 8163.091272000202,
    "total_rag_time_ms": 5042.144005000409
  },
  {
    "question_id": 3,
    "question": "What is model compression and why is it important?",
    "ground_truth": "Model compression involves transforming a large, resource-intensive model into a compact version suitable for deployment on resource-constrained devices. It is important because it can enhance LLM inference speed, optimize resource efficiency, and make LLMs more accessible for practical deployment.",
    "rag_answer": "Model compression refers to the process of reducing the size of large language models (LLMs) while maintaining their accuracy and performance. It is important because it enables efficient deployment of LLMs on limited computational resources, which is essential for practical applications such as mobile devices or edge computing systems.",
    "no_rag_answer": "Model compression refers to the process of reducing the size, complexity or both of a machine learning model without significantly sacrificing its accuracy. This technique is important for several reasons: 1) Reducing storage requirements: Large models require a lot of storage space which can be a challenge when deploying models on edge devices with limited memory. Compressing the model reduces its storage footprint, making it easier to store and deploy. 2) Improving inference time: Complex models take longer to run inference, which can be a bottleneck in many applications.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8946562756081846,
      0.8690318793121534,
      0.8566234871059848
    ],
    "avg_context_score": 0.8734372140087743,
    "full_context": "LLM Infer ence and Deployment The ef ficiency of compressed LLMs during deployment is also a significant area for exploration. This involves multiple evaluation metrics, including arithmetic intensity , memory size, and throughput. Furthermore, we can use an analytical tool, the Roofline Model, to assess the resource ef ficiency of compressed LLMs on specific hardware. Evaluating the deployment ef ficiency of compressed LLMs on specific hardware can guide researchers in selecting and analyzing the advantages and disadvantages of various model compression methods and further optimizing these methods. The Effect of Scaling Law The scaling law underscores the significant impact of model size, dataset size, and compute resources on the performance of LLMs. However , the scaling law presents a fundamental challenge for LLM compression, i.e., there is a trade-of f between model size and performance in compressed LLMs. Delving into the mechanisms and theories underpinning the scaling law is crucial for elucidating and potentially overcoming this limitation. AutoML for LLM Compr ession Existing compression techniques have made remarkable progress, but they still heavily depend on manual design. For instance, designing appropriate student architectures for knowledge distillation requires a significant amount of human ef fort. To reduce this reliance on manual design, a feasible solution is to combine Automated Machine Learning (AutoML) techniques such as Meta-Learning and Neural Architecture Search (NAS) with model compression. By combining with AutoML techniques, model compression can automatically select appropriate hyperparameters and tailor architectures and scales of compressed models, thus minimizing human involvement and lowering the associated costs. Furthermore, AutoML can identify optimal model compression strategies tailored to specific task requirements, thereby further enhancing compression rates without compromising model performance. Explainability of LLM Compr ession Earlier research has raised significant concerns regarding the explainability of model compression techniques applied to Pre-trained Language Models (PLMs). Notably , these same challenges extend to LLM compression methods as well. For example, CoT -distillation can enhance SLMs' reasoning performance, yet the mechanism through which it imparts CoT ability remains unclear . This challenge underscores the importance of integrating explainability with model compression approaches for the advancement of LLM compression applications. Explainability not only clarifies the changes and trade-of fs in the compression process but also enhances efficiency and accuracy . Additionally , interpretability aids in evaluating the compressed model's performance to ensure it aligns with practical requirements. Conclusion In the survey , we have explored model compression techniques for LLMs. Our coverage spanned compression\n\nincludes quantization, pruning, knowledge distillation, and low-rank factorization. Furthermore, our study sheds light on prevailing challenges and of fers a glimpse into potential future research trajectories in this evolving field. We advocate for collaborative ef forts within the community to pave the way for an ecologically conscious, all-encompassing, and sustainable future for LLMs. While there were previous surveys on neural networks model compression and it has been lightly discussed in prior surveys on LMs and LLMs, our work is the inaugural survey dedicated solely to model compression for LLMs. Metrics and Benchmarks Metrics Model compression of LLMs can be measured using various metrics, which capture dif ferent aspects of performance. These metrics are commonly presented alongside accuracy and zero-shot ability to comprehensively evaluate the LLM. Model Size in a LLM typically is measured by the number of total parameters of the LLM. In general, LLMs with more parameters often requires more computational resources and memory for both training and inference. Floating Point Operations (FLOPs) is an indicator that measures the computational ef ficiency of LLMs, representing the number of floating-point operations required for the LLM to perform an instance. In model compression, reducing FLOPs helps to make the LLM run faster and more ef ficiently . Mean FLOPS Utilization (MFU) quantifies the practical ef ficiency of computational resource utilization by LLMs during tasks. MFU measures the ratio of actual FLOPS utilized by the LLM to the maximum theoretical FLOPS of a device. Unlike FLOPs, which estimates the maximum operations an LLM might perform, MFU assesses the actual ef fectiveness of resource use in operation. Essentially , while FLOPs measures a LLM's theoretical compute needs, MFU shows how ef fectively these computations are utilized in practice. Infer ence time (i.e., latency) measures the time taken by the LLM to process and generate responses for input data during inference. Inference time is particularly crucial for real-world applications where the LLM needs to respond for user queries or process lar ge amounts of data in real-time. Speedup Ratio measures how much faster a compressed LLM performs tasks compared to the original LLM. Specifically , it measures the ratio of the inference time of the uncompressed model over the inference time of the compressed model. Higher ratios mean greater ef ficiency and reduced computation time, highlighting effective compression. Compr ession Ratio measures how much a LLM's size is reduced through compression, calculated as the original size divided by the compressed size. Higher ratios mean greater size reduction, showing the compression's ef fectiveness in saving storage and memory .\n\nA Survey on Model Compr ession for Large Language Models Authors: Xunyu Zhu, Jian Li (Corresponding author), Yong Liu, Can Ma, Weiping Wang Affiliations: Institute of Information Engineering, Chinese Academy of Sciences School of Cyber Security , University of Chinese Academy of Sciences Gaoling School of Artificial Intelligence, Renmin University of China Contact: zhuxunyu@iie.ac.cn , lijian9026@iie.ac.cn , macan@iie.ac.cn , wangweiping@iie.ac.cn , liuyonggsai@ruc.edu.cn Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully . Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emer ged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey of fers valuable insights for researchers and practitioners, aiming to enhance ef ficiency and real-world applicability of LLMs while laying a foundation for future advancements. Introduction Large Language Models (LLMs) refer to Transformer language models that contain billions (or more) of parameters, which are trained on massive text data. LLMs consistently exhibit remarkable performance across various tasks, but their exceptional capabilities come with significant challenges stemming from their extensive size and computational requirements. For instance, the GPT -175B model, with an impressive 175 billion parameters, demands a minimum of 350GB of memory in half-precision (FP16) format. Furthermore, deploying this model for inference necessitates at least five A100 GPUs, each featuring 80GB of memory , to ef ficiently manage operations. To tackle these issues, a prevalent approach known as model compression of fers a solution. Model compression involves transforming a lar ge, resource-intensive model into a compact version suitable for deployment on resource-constrained devices. Additionally , model compression can enhance LLM inference speed and optimizes resource ef ficiency . In our paper , our primary objective is to illuminate the recent strides made in the domain of model compression techniques tailored specifically for LLMs. Our work conducts an exhaustive survey of methodologies, metrics, and benchmarks of model compression for LLMs. The taxonomy of model compression methods for LLMs",
    "context_length_chars": 8329,
    "rag_answer_length_words": 49,
    "no_rag_answer_length_words": 89,
    "retrieval_time_ms": 23.750411999571952,
    "rag_generation_time_ms": 5646.0468639998,
    "no_rag_generation_time_ms": 8168.527035999432,
    "total_rag_time_ms": 5669.797275999372
  },
  {
    "question_id": 4,
    "question": "What are the main categories of model compression techniques covered in this survey?",
    "ground_truth": "The survey covers four main categories: (1) Quantization, (2) Pruning, (3) Knowledge Distillation, and (4) Low-Rank Factorization.",
    "rag_answer": "",
    "no_rag_answer": "The main categories of model compression techniques covered in this survey include: 1. Pruning and quantization: These techniques involve removing unnecessary connections or weights from a model, and reducing the precision of the remaining weights to reduce the size of the model. Knowledge distillation: This technique involves training a smaller student model to mimic the behavior of a larger teacher model, allowing for transfer learning and reduced model size. Low-rank factorization: This technique involves decomposing the weight matrices of a model into lower-dimensional factors, which can be stored more efficiently.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8975371747020446,
      0.9026745855553528,
      0.8949147410349365
    ],
    "avg_context_score": 0.898375500430778,
    "full_context": "A Survey on Model Compr ession for Large Language Models Authors: Xunyu Zhu, Jian Li (Corresponding author), Yong Liu, Can Ma, Weiping Wang Affiliations: Institute of Information Engineering, Chinese Academy of Sciences School of Cyber Security , University of Chinese Academy of Sciences Gaoling School of Artificial Intelligence, Renmin University of China Contact: zhuxunyu@iie.ac.cn , lijian9026@iie.ac.cn , macan@iie.ac.cn , wangweiping@iie.ac.cn , liuyonggsai@ruc.edu.cn Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully . Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emer ged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey of fers valuable insights for researchers and practitioners, aiming to enhance ef ficiency and real-world applicability of LLMs while laying a foundation for future advancements. Introduction Large Language Models (LLMs) refer to Transformer language models that contain billions (or more) of parameters, which are trained on massive text data. LLMs consistently exhibit remarkable performance across various tasks, but their exceptional capabilities come with significant challenges stemming from their extensive size and computational requirements. For instance, the GPT -175B model, with an impressive 175 billion parameters, demands a minimum of 350GB of memory in half-precision (FP16) format. Furthermore, deploying this model for inference necessitates at least five A100 GPUs, each featuring 80GB of memory , to ef ficiently manage operations. To tackle these issues, a prevalent approach known as model compression of fers a solution. Model compression involves transforming a lar ge, resource-intensive model into a compact version suitable for deployment on resource-constrained devices. Additionally , model compression can enhance LLM inference speed and optimizes resource ef ficiency . In our paper , our primary objective is to illuminate the recent strides made in the domain of model compression techniques tailored specifically for LLMs. Our work conducts an exhaustive survey of methodologies, metrics, and benchmarks of model compression for LLMs. The taxonomy of model compression methods for LLMs\n\nLLM Infer ence and Deployment The ef ficiency of compressed LLMs during deployment is also a significant area for exploration. This involves multiple evaluation metrics, including arithmetic intensity , memory size, and throughput. Furthermore, we can use an analytical tool, the Roofline Model, to assess the resource ef ficiency of compressed LLMs on specific hardware. Evaluating the deployment ef ficiency of compressed LLMs on specific hardware can guide researchers in selecting and analyzing the advantages and disadvantages of various model compression methods and further optimizing these methods. The Effect of Scaling Law The scaling law underscores the significant impact of model size, dataset size, and compute resources on the performance of LLMs. However , the scaling law presents a fundamental challenge for LLM compression, i.e., there is a trade-of f between model size and performance in compressed LLMs. Delving into the mechanisms and theories underpinning the scaling law is crucial for elucidating and potentially overcoming this limitation. AutoML for LLM Compr ession Existing compression techniques have made remarkable progress, but they still heavily depend on manual design. For instance, designing appropriate student architectures for knowledge distillation requires a significant amount of human ef fort. To reduce this reliance on manual design, a feasible solution is to combine Automated Machine Learning (AutoML) techniques such as Meta-Learning and Neural Architecture Search (NAS) with model compression. By combining with AutoML techniques, model compression can automatically select appropriate hyperparameters and tailor architectures and scales of compressed models, thus minimizing human involvement and lowering the associated costs. Furthermore, AutoML can identify optimal model compression strategies tailored to specific task requirements, thereby further enhancing compression rates without compromising model performance. Explainability of LLM Compr ession Earlier research has raised significant concerns regarding the explainability of model compression techniques applied to Pre-trained Language Models (PLMs). Notably , these same challenges extend to LLM compression methods as well. For example, CoT -distillation can enhance SLMs' reasoning performance, yet the mechanism through which it imparts CoT ability remains unclear . This challenge underscores the importance of integrating explainability with model compression approaches for the advancement of LLM compression applications. Explainability not only clarifies the changes and trade-of fs in the compression process but also enhances efficiency and accuracy . Additionally , interpretability aids in evaluating the compressed model's performance to ensure it aligns with practical requirements. Conclusion In the survey , we have explored model compression techniques for LLMs. Our coverage spanned compression\n\nincludes quantization, pruning, knowledge distillation, and low-rank factorization. Furthermore, our study sheds light on prevailing challenges and of fers a glimpse into potential future research trajectories in this evolving field. We advocate for collaborative ef forts within the community to pave the way for an ecologically conscious, all-encompassing, and sustainable future for LLMs. While there were previous surveys on neural networks model compression and it has been lightly discussed in prior surveys on LMs and LLMs, our work is the inaugural survey dedicated solely to model compression for LLMs. Metrics and Benchmarks Metrics Model compression of LLMs can be measured using various metrics, which capture dif ferent aspects of performance. These metrics are commonly presented alongside accuracy and zero-shot ability to comprehensively evaluate the LLM. Model Size in a LLM typically is measured by the number of total parameters of the LLM. In general, LLMs with more parameters often requires more computational resources and memory for both training and inference. Floating Point Operations (FLOPs) is an indicator that measures the computational ef ficiency of LLMs, representing the number of floating-point operations required for the LLM to perform an instance. In model compression, reducing FLOPs helps to make the LLM run faster and more ef ficiently . Mean FLOPS Utilization (MFU) quantifies the practical ef ficiency of computational resource utilization by LLMs during tasks. MFU measures the ratio of actual FLOPS utilized by the LLM to the maximum theoretical FLOPS of a device. Unlike FLOPs, which estimates the maximum operations an LLM might perform, MFU assesses the actual ef fectiveness of resource use in operation. Essentially , while FLOPs measures a LLM's theoretical compute needs, MFU shows how ef fectively these computations are utilized in practice. Infer ence time (i.e., latency) measures the time taken by the LLM to process and generate responses for input data during inference. Inference time is particularly crucial for real-world applications where the LLM needs to respond for user queries or process lar ge amounts of data in real-time. Speedup Ratio measures how much faster a compressed LLM performs tasks compared to the original LLM. Specifically , it measures the ratio of the inference time of the uncompressed model over the inference time of the compressed model. Higher ratios mean greater ef ficiency and reduced computation time, highlighting effective compression. Compr ession Ratio measures how much a LLM's size is reduced through compression, calculated as the original size divided by the compressed size. Higher ratios mean greater size reduction, showing the compression's ef fectiveness in saving storage and memory .",
    "context_length_chars": 8329,
    "rag_answer_length_words": 0,
    "no_rag_answer_length_words": 90,
    "retrieval_time_ms": 30.189905999577604,
    "rag_generation_time_ms": 4012.8658889998405,
    "no_rag_generation_time_ms": 8153.055122999831,
    "total_rag_time_ms": 4043.055794999418
  },
  {
    "question_id": 5,
    "question": "What metrics are used to evaluate compressed LLMs?",
    "ground_truth": "Key metrics include: Model Size (number of parameters), FLOPs (floating-point operations), MFU (Mean FLOPS Utilization), Inference Time/Latency, Speedup Ratio, and Compression Ratio.",
    "rag_answer": "",
    "no_rag_answer": "Compressed language models (LLMs) can be evaluated using various metrics depending on the specific task or application. Some common metrics used for evaluating compressed LLMs include: 1. Perplexity: This is a commonly used metric for evaluating the quality of an LLM's predictions. It measures how well the model predicts a given text sequence.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8897594052796318,
      0.8459029635567319,
      0.8461634674932768
    ],
    "avg_context_score": 0.8606086121098802,
    "full_context": "includes quantization, pruning, knowledge distillation, and low-rank factorization. Furthermore, our study sheds light on prevailing challenges and of fers a glimpse into potential future research trajectories in this evolving field. We advocate for collaborative ef forts within the community to pave the way for an ecologically conscious, all-encompassing, and sustainable future for LLMs. While there were previous surveys on neural networks model compression and it has been lightly discussed in prior surveys on LMs and LLMs, our work is the inaugural survey dedicated solely to model compression for LLMs. Metrics and Benchmarks Metrics Model compression of LLMs can be measured using various metrics, which capture dif ferent aspects of performance. These metrics are commonly presented alongside accuracy and zero-shot ability to comprehensively evaluate the LLM. Model Size in a LLM typically is measured by the number of total parameters of the LLM. In general, LLMs with more parameters often requires more computational resources and memory for both training and inference. Floating Point Operations (FLOPs) is an indicator that measures the computational ef ficiency of LLMs, representing the number of floating-point operations required for the LLM to perform an instance. In model compression, reducing FLOPs helps to make the LLM run faster and more ef ficiently . Mean FLOPS Utilization (MFU) quantifies the practical ef ficiency of computational resource utilization by LLMs during tasks. MFU measures the ratio of actual FLOPS utilized by the LLM to the maximum theoretical FLOPS of a device. Unlike FLOPs, which estimates the maximum operations an LLM might perform, MFU assesses the actual ef fectiveness of resource use in operation. Essentially , while FLOPs measures a LLM's theoretical compute needs, MFU shows how ef fectively these computations are utilized in practice. Infer ence time (i.e., latency) measures the time taken by the LLM to process and generate responses for input data during inference. Inference time is particularly crucial for real-world applications where the LLM needs to respond for user queries or process lar ge amounts of data in real-time. Speedup Ratio measures how much faster a compressed LLM performs tasks compared to the original LLM. Specifically , it measures the ratio of the inference time of the uncompressed model over the inference time of the compressed model. Higher ratios mean greater ef ficiency and reduced computation time, highlighting effective compression. Compr ession Ratio measures how much a LLM's size is reduced through compression, calculated as the original size divided by the compressed size. Higher ratios mean greater size reduction, showing the compression's ef fectiveness in saving storage and memory .\n\nBenchmarks and Datasets The main goal of these benchmarks and datasets is to measure the ef ficiency and performance of compressed LLMs in comparison to their uncompressed counterparts. These benchmarks and datasets typically consist of diverse tasks and datasets that cover a range of natural language processing challenges. Common Benchmarks and Datasets The majority of research evaluates compressed LLMs on well-established NLP benchmarks and datasets. For instance, WikiText-2, C4, and PTB are designed for evaluating the perplexity performance of language models. LAMBADA, PIQA, and OpenBookQA are designed to evaluate the zero-shot ability of language models. GSM8K, CommonsenseQA and StrategyQA are designed to evaluate the reasoning ability of language models. BIG-Bench BIG-Bench (BBH) is a benchmark suite designed for LLMs, covering over 200 NLP tasks, e.g., Text Comprehension Tasks, Inference Tasks, Mathematical Reasoning Tasks. The aim of BBH is to evaluate the performance of LLMs across these various complex tasks. The compressed LLMs use BBH to measure their capability across a multidimensional spectrum of tasks. Unseen Instructions Datasets Unseen instructions datasets are used to evaluate the performance of LLMs on unseen tasks. For instance, the Vicuna-Instructions dataset created by GPT -4 includes 80 complex questions across nine dif ferent categories like generic, knowledge-based, and writing tasks. Another dataset, User -Oriented-Instructions, consists of 252 carefully selected instructions inspired by various user -focused applications such as Grammarly , StackOverflow , and Overleaf. These datasets evaluate how well compact LLMs can handle and carry out new tasks by presenting them with unfamiliar instructions. EleutherAI LM Harness The EleutherAI LM Harness is an advanced framework for evaluating LLMs, providing a unified testing platform that supports over 60 standard academic benchmarks along with hundreds of subtasks and variants. The standardized evaluation tasks provided by the harness ensure the reproducibility and comparability of evaluation, which is essential for implementing fair and reproducible evaluations for the compressed LLMs. Quantization Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameters of the model with minimal loss in inference performance. Quantization can be categorized into two main approaches: Quantization-A ware Training (QA T), and Post-T raining Quantization (PTQ). The primary distinction between\n\nA Survey on Model Compr ession for Large Language Models Authors: Xunyu Zhu, Jian Li (Corresponding author), Yong Liu, Can Ma, Weiping Wang Affiliations: Institute of Information Engineering, Chinese Academy of Sciences School of Cyber Security , University of Chinese Academy of Sciences Gaoling School of Artificial Intelligence, Renmin University of China Contact: zhuxunyu@iie.ac.cn , lijian9026@iie.ac.cn , macan@iie.ac.cn , wangweiping@iie.ac.cn , liuyonggsai@ruc.edu.cn Abstract Large Language Models (LLMs) have transformed natural language processing tasks successfully . Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emer ged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey of fers valuable insights for researchers and practitioners, aiming to enhance ef ficiency and real-world applicability of LLMs while laying a foundation for future advancements. Introduction Large Language Models (LLMs) refer to Transformer language models that contain billions (or more) of parameters, which are trained on massive text data. LLMs consistently exhibit remarkable performance across various tasks, but their exceptional capabilities come with significant challenges stemming from their extensive size and computational requirements. For instance, the GPT -175B model, with an impressive 175 billion parameters, demands a minimum of 350GB of memory in half-precision (FP16) format. Furthermore, deploying this model for inference necessitates at least five A100 GPUs, each featuring 80GB of memory , to ef ficiently manage operations. To tackle these issues, a prevalent approach known as model compression of fers a solution. Model compression involves transforming a lar ge, resource-intensive model into a compact version suitable for deployment on resource-constrained devices. Additionally , model compression can enhance LLM inference speed and optimizes resource ef ficiency . In our paper , our primary objective is to illuminate the recent strides made in the domain of model compression techniques tailored specifically for LLMs. Our work conducts an exhaustive survey of methodologies, metrics, and benchmarks of model compression for LLMs. The taxonomy of model compression methods for LLMs",
    "context_length_chars": 7933,
    "rag_answer_length_words": 0,
    "no_rag_answer_length_words": 53,
    "retrieval_time_ms": 23.86815000045317,
    "rag_generation_time_ms": 10654.787467999995,
    "no_rag_generation_time_ms": 8134.547700999974,
    "total_rag_time_ms": 10678.655618000448
  },
  {
    "question_id": 6,
    "question": "What is the difference between FLOPs and MFU?",
    "ground_truth": "FLOPs measures the theoretical computational efficiency (number of floating-point operations required), while MFU (Mean FLOPS Utilization) measures the practical efficiency by calculating the ratio of actual FLOPS utilized to the maximum theoretical FLOPS of a device. FLOPs shows theoretical compute needs, while MFU shows actual effectiveness of resource use.",
    "rag_answer": "FLOPs and MFU are both metrics used to evaluate the efficiency of machine learning models, but they serve different purposes. FLOPs stands for Floating Point Operations Per Second and measures the computational power required to run a model. It is commonly used to compare the speed and efficiency of different hardware accelerators or architectures. On the other hand, MFU stands for Mean Field Utility and is a metric used to evaluate the quality of reinforcement learning policies.",
    "no_rag_answer": "FLOPs (floating-point operations per second) and MFU (most frequently used) are two different concepts in computing. FLOPs refer to the number of floating-point arithmetic operations that a computer can perform in one second. This is often used as a measure of a computer's performance, particularly for tasks such as machine learning or scientific simulations that require a lot of mathematical computations. MFU, on the other hand, refers to a memory management technique where the most frequently accessed data is stored closer to the CPU to improve performance.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.6814571158544993,
      0.669222871831316,
      0.6773683258721732
    ],
    "avg_context_score": 0.6760161045193295,
    "full_context": "Benchmarks and Datasets The main goal of these benchmarks and datasets is to measure the ef ficiency and performance of compressed LLMs in comparison to their uncompressed counterparts. These benchmarks and datasets typically consist of diverse tasks and datasets that cover a range of natural language processing challenges. Common Benchmarks and Datasets The majority of research evaluates compressed LLMs on well-established NLP benchmarks and datasets. For instance, WikiText-2, C4, and PTB are designed for evaluating the perplexity performance of language models. LAMBADA, PIQA, and OpenBookQA are designed to evaluate the zero-shot ability of language models. GSM8K, CommonsenseQA and StrategyQA are designed to evaluate the reasoning ability of language models. BIG-Bench BIG-Bench (BBH) is a benchmark suite designed for LLMs, covering over 200 NLP tasks, e.g., Text Comprehension Tasks, Inference Tasks, Mathematical Reasoning Tasks. The aim of BBH is to evaluate the performance of LLMs across these various complex tasks. The compressed LLMs use BBH to measure their capability across a multidimensional spectrum of tasks. Unseen Instructions Datasets Unseen instructions datasets are used to evaluate the performance of LLMs on unseen tasks. For instance, the Vicuna-Instructions dataset created by GPT -4 includes 80 complex questions across nine dif ferent categories like generic, knowledge-based, and writing tasks. Another dataset, User -Oriented-Instructions, consists of 252 carefully selected instructions inspired by various user -focused applications such as Grammarly , StackOverflow , and Overleaf. These datasets evaluate how well compact LLMs can handle and carry out new tasks by presenting them with unfamiliar instructions. EleutherAI LM Harness The EleutherAI LM Harness is an advanced framework for evaluating LLMs, providing a unified testing platform that supports over 60 standard academic benchmarks along with hundreds of subtasks and variants. The standardized evaluation tasks provided by the harness ensure the reproducibility and comparability of evaluation, which is essential for implementing fair and reproducible evaluations for the compressed LLMs. Quantization Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameters of the model with minimal loss in inference performance. Quantization can be categorized into two main approaches: Quantization-A ware Training (QA T), and Post-T raining Quantization (PTQ). The primary distinction between\n\nsame quantization parameters to quantize the values in each cluster , into layer normalization and linear layer weights to ef ficiently reduce the ef fect of numerical range dif ferences between channels. OliV e thinks that outliers are more important than the normal values, and uses an outlier -victim pair (OVP) quantization to handle outlier values locally with low hardware overheads and significant performance benefits. OS+ further finds that outliers are concentrated in specific and asymmetric channels. Based on the findings, OS+ incorporates channel- wise shifting to eliminate the impact of asymmetry and channel-wise scaling to balance the distribution of outliers. LLM-FP4 uses floating-point formats (specifically FP8 and FP4) to address the limitations of traditional integer quantization (such as INT8 and INT4) to deal with outliers. Furthermore, LLM-FP4 points out that exponent bits and clipping range are important factors that ef fect the performance of FP quantization, and introduces a search-based framework for determining the optimal exponent bias and maximal quantization value. OmniQuant handles the activation outliers by equivalently shifting the challenge of quantization from activations to weights, and optimizes the clipping threshold to adjust the extreme values of the weights. KV Cache Quantization With the increasing number of input tokens supported by LLMs, the memory usage of the KV cache also increases. Recent ef forts begin to focus on kv cache quantization to reduce the memory footprint of LLMs and accelerate their inference. For example, KVQuant proposes several KV Cache Quantization methods, such as Per-Channel Key Quantization, PreRoPE Key Quantization, and Non-Uniform kv cache quantization, to implement 10 million context length LLM inference. Through an in-depth analysis of the element distribution within the KV cache, KIVI finds that key caches should be quantized per -channel, while value caches should be quantized per -token. Finally , KIVI succeeds in quantizing the KV cache to 2 bits without fine-tuning. WKVQuant presents an innovative approach for quantizing lar ge language models (LLMs) by integrating past- only quantization to refine attention computations, employing a two-dimensional quantization strategy to manage the distribution of key/value (KV) caches ef fectively , and utilizing cross-block reconstruction regularization for optimizing parameters. This method enables the quantization of both weights and KV caches, resulting in memory savings that rival those of weight-activation quantization, while nearly matching the performance levels of weight-only quantization. Pruning Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant components. Pruning can be divided into Unstructured Pruning, Semi-Structured Pruning, and Structured Pruning. Structured pruning removes entire components like neurons, attention heads, or layers based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. Semi-structured pruning is a method that lies between structured pruning and unstructured pruning, capable of achieving fine-grained pruning and structural regularization simultaneously . It prunes partial parameters based on specific patterns rather than entire channels, filters, or neurons, making it a fine-grained form of structured pruning.\n\nincludes quantization, pruning, knowledge distillation, and low-rank factorization. Furthermore, our study sheds light on prevailing challenges and of fers a glimpse into potential future research trajectories in this evolving field. We advocate for collaborative ef forts within the community to pave the way for an ecologically conscious, all-encompassing, and sustainable future for LLMs. While there were previous surveys on neural networks model compression and it has been lightly discussed in prior surveys on LMs and LLMs, our work is the inaugural survey dedicated solely to model compression for LLMs. Metrics and Benchmarks Metrics Model compression of LLMs can be measured using various metrics, which capture dif ferent aspects of performance. These metrics are commonly presented alongside accuracy and zero-shot ability to comprehensively evaluate the LLM. Model Size in a LLM typically is measured by the number of total parameters of the LLM. In general, LLMs with more parameters often requires more computational resources and memory for both training and inference. Floating Point Operations (FLOPs) is an indicator that measures the computational ef ficiency of LLMs, representing the number of floating-point operations required for the LLM to perform an instance. In model compression, reducing FLOPs helps to make the LLM run faster and more ef ficiently . Mean FLOPS Utilization (MFU) quantifies the practical ef ficiency of computational resource utilization by LLMs during tasks. MFU measures the ratio of actual FLOPS utilized by the LLM to the maximum theoretical FLOPS of a device. Unlike FLOPs, which estimates the maximum operations an LLM might perform, MFU assesses the actual ef fectiveness of resource use in operation. Essentially , while FLOPs measures a LLM's theoretical compute needs, MFU shows how ef fectively these computations are utilized in practice. Infer ence time (i.e., latency) measures the time taken by the LLM to process and generate responses for input data during inference. Inference time is particularly crucial for real-world applications where the LLM needs to respond for user queries or process lar ge amounts of data in real-time. Speedup Ratio measures how much faster a compressed LLM performs tasks compared to the original LLM. Specifically , it measures the ratio of the inference time of the uncompressed model over the inference time of the compressed model. Higher ratios mean greater ef ficiency and reduced computation time, highlighting effective compression. Compr ession Ratio measures how much a LLM's size is reduced through compression, calculated as the original size divided by the compressed size. Higher ratios mean greater size reduction, showing the compression's ef fectiveness in saving storage and memory .",
    "context_length_chars": 8820,
    "rag_answer_length_words": 77,
    "no_rag_answer_length_words": 87,
    "retrieval_time_ms": 24.89200800027902,
    "rag_generation_time_ms": 10631.588811000256,
    "no_rag_generation_time_ms": 8141.530095000235,
    "total_rag_time_ms": 10656.480819000535
  },
  {
    "question_id": 7,
    "question": "What is quantization in the context of LLMs?",
    "ground_truth": "Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameters of the model with minimal loss in inference performance.",
    "rag_answer": "In the context of LLMs, quantization refers to the process of reducing the precision of model parameters (such as weights and activations) to improve computational efficiency and reduce memory usage. This can involve using different quantization techniques, such as integer or floating-point quantization, channel-wise shifting and scaling, and outlier handling methods like OVP, LLM-FP4, OmniQuant, and KVCache Quantization.",
    "no_rag_answer": "Quantization refers to the process of reducing the precision or accuracy of a model's parameters, weights, and activations during training. This can be done for various reasons such as memory efficiency, faster computation time, and improved generalization performance. In the context of LLMs (Language Models), quantization can involve converting floating-point numbers used in the model into lower-precision formats like integers or fixed-point numbers.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8345157904813902,
      0.8867962150296123,
      0.8325042710802109
    ],
    "avg_context_score": 0.8512720921970711,
    "full_context": "same quantization parameters to quantize the values in each cluster , into layer normalization and linear layer weights to ef ficiently reduce the ef fect of numerical range dif ferences between channels. OliV e thinks that outliers are more important than the normal values, and uses an outlier -victim pair (OVP) quantization to handle outlier values locally with low hardware overheads and significant performance benefits. OS+ further finds that outliers are concentrated in specific and asymmetric channels. Based on the findings, OS+ incorporates channel- wise shifting to eliminate the impact of asymmetry and channel-wise scaling to balance the distribution of outliers. LLM-FP4 uses floating-point formats (specifically FP8 and FP4) to address the limitations of traditional integer quantization (such as INT8 and INT4) to deal with outliers. Furthermore, LLM-FP4 points out that exponent bits and clipping range are important factors that ef fect the performance of FP quantization, and introduces a search-based framework for determining the optimal exponent bias and maximal quantization value. OmniQuant handles the activation outliers by equivalently shifting the challenge of quantization from activations to weights, and optimizes the clipping threshold to adjust the extreme values of the weights. KV Cache Quantization With the increasing number of input tokens supported by LLMs, the memory usage of the KV cache also increases. Recent ef forts begin to focus on kv cache quantization to reduce the memory footprint of LLMs and accelerate their inference. For example, KVQuant proposes several KV Cache Quantization methods, such as Per-Channel Key Quantization, PreRoPE Key Quantization, and Non-Uniform kv cache quantization, to implement 10 million context length LLM inference. Through an in-depth analysis of the element distribution within the KV cache, KIVI finds that key caches should be quantized per -channel, while value caches should be quantized per -token. Finally , KIVI succeeds in quantizing the KV cache to 2 bits without fine-tuning. WKVQuant presents an innovative approach for quantizing lar ge language models (LLMs) by integrating past- only quantization to refine attention computations, employing a two-dimensional quantization strategy to manage the distribution of key/value (KV) caches ef fectively , and utilizing cross-block reconstruction regularization for optimizing parameters. This method enables the quantization of both weights and KV caches, resulting in memory savings that rival those of weight-activation quantization, while nearly matching the performance levels of weight-only quantization. Pruning Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant components. Pruning can be divided into Unstructured Pruning, Semi-Structured Pruning, and Structured Pruning. Structured pruning removes entire components like neurons, attention heads, or layers based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. Semi-structured pruning is a method that lies between structured pruning and unstructured pruning, capable of achieving fine-grained pruning and structural regularization simultaneously . It prunes partial parameters based on specific patterns rather than entire channels, filters, or neurons, making it a fine-grained form of structured pruning.\n\nthe two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization\n\nWeight-only quantization is the most conventional and widespread method. For example, LUT -GEMM uses binary-coding quantization (BCQ) format, which factorizes the parameters of LLMs into binary parameters and a set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ proposes a layer -wise quantization method based on Optimal Brain Quantization (OBQ), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from a calibration set, and multiplies weight and Hessian matrices with a Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important ef fect on LLMs' quantization performance, and store these sensitive weights in high precision. For example, AWQ stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates a per -channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR employs the L2 error between the original and quantized predictions as a weight sensitivity metric. Furthermore, SqueezeLLM introduces a weights clusters algorithm based on sensitivity , using k-means centroids as quantized weight values, to identify sensitive weights. The sensitivity is approximated by the Hessian matrix of weights. Then, SqueezeLLM stores sensitive weights in an ef ficient sparse format, and quantize other weights. SqueezeLLM quantizes LLMs in 3-bit, and achieves a more than 2x speedup compared to the FP16 baseline. Weight-Activation Quantization Alongside works centered on weight-only quantization in LLMs, there is a plethora of research focusing primarily on weight-activation quantization in LLMs. For example, ZeroQuant is the first work to implement weight-activation quantization for LLMs, which uses group-wise quantization for weight and token-wise quantization for activations, and reduces the precision for weights and activations of LLMs to INT8. LLMs have outliers in activations, and the performance of LLMs declines a lot, if these activations with outliers are directly quantized. Recent works try to treat these outliers specially to reduce quantization errors in weight- activation quantization. For example, LLM.int8() stores these outlier feature dimensions into high-precision, and uses vector -wise quantization, which assigns separate normalization constants to each inner product within matrix multiplication, to quantize other features. LLM.int8() quantizes weights and activations of LLMs into 8- bit without any performance degradation. SmoothQuant designs a per -channel scaling transformation to smooths the activation outliers based on the discovery that dif ferent tokens have similar variations across channels of activations. RPTQ finds that the range of values varies greatly between dif ferent channels, and integrates a channel reordering method, which clusters and reorders the channels in the activation and uses the",
    "context_length_chars": 10243,
    "rag_answer_length_words": 58,
    "no_rag_answer_length_words": 63,
    "retrieval_time_ms": 24.389029999838385,
    "rag_generation_time_ms": 8578.42790199993,
    "no_rag_generation_time_ms": 8143.988036999872,
    "total_rag_time_ms": 8602.816931999769
  },
  {
    "question_id": 8,
    "question": "What is the difference between QAT and PTQ?",
    "ground_truth": "The primary distinction lies in whether retraining is needed during quantization. Post-Training Quantization (PTQ) enables direct use of quantized models in inference without retraining, while Quantization-Aware Training (QAT) requires retraining to rectify errors introduced by quantization.",
    "rag_answer": "",
    "no_rag_answer": "QAT stands for Quality Assurance Testing, which is a process of testing software or systems to ensure that they meet specified quality standards. This includes identifying defects and ensuring that the product meets customer requirements and expectations. PTQ stands for Performance Testing Qualification, which is a certification program offered by Microsoft. It is designed to help IT professionals demonstrate their expertise in performance testing and troubleshooting.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.7632017854996818,
      0.6483770862343761,
      0.6511314546656006
    ],
    "avg_context_score": 0.6875701087998861,
    "full_context": "the two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization\n\nBenchmarks and Datasets The main goal of these benchmarks and datasets is to measure the ef ficiency and performance of compressed LLMs in comparison to their uncompressed counterparts. These benchmarks and datasets typically consist of diverse tasks and datasets that cover a range of natural language processing challenges. Common Benchmarks and Datasets The majority of research evaluates compressed LLMs on well-established NLP benchmarks and datasets. For instance, WikiText-2, C4, and PTB are designed for evaluating the perplexity performance of language models. LAMBADA, PIQA, and OpenBookQA are designed to evaluate the zero-shot ability of language models. GSM8K, CommonsenseQA and StrategyQA are designed to evaluate the reasoning ability of language models. BIG-Bench BIG-Bench (BBH) is a benchmark suite designed for LLMs, covering over 200 NLP tasks, e.g., Text Comprehension Tasks, Inference Tasks, Mathematical Reasoning Tasks. The aim of BBH is to evaluate the performance of LLMs across these various complex tasks. The compressed LLMs use BBH to measure their capability across a multidimensional spectrum of tasks. Unseen Instructions Datasets Unseen instructions datasets are used to evaluate the performance of LLMs on unseen tasks. For instance, the Vicuna-Instructions dataset created by GPT -4 includes 80 complex questions across nine dif ferent categories like generic, knowledge-based, and writing tasks. Another dataset, User -Oriented-Instructions, consists of 252 carefully selected instructions inspired by various user -focused applications such as Grammarly , StackOverflow , and Overleaf. These datasets evaluate how well compact LLMs can handle and carry out new tasks by presenting them with unfamiliar instructions. EleutherAI LM Harness The EleutherAI LM Harness is an advanced framework for evaluating LLMs, providing a unified testing platform that supports over 60 standard academic benchmarks along with hundreds of subtasks and variants. The standardized evaluation tasks provided by the harness ensure the reproducibility and comparability of evaluation, which is essential for implementing fair and reproducible evaluations for the compressed LLMs. Quantization Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameters of the model with minimal loss in inference performance. Quantization can be categorized into two main approaches: Quantization-A ware Training (QA T), and Post-T raining Quantization (PTQ). The primary distinction between\n\nWeight-only quantization is the most conventional and widespread method. For example, LUT -GEMM uses binary-coding quantization (BCQ) format, which factorizes the parameters of LLMs into binary parameters and a set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ proposes a layer -wise quantization method based on Optimal Brain Quantization (OBQ), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from a calibration set, and multiplies weight and Hessian matrices with a Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important ef fect on LLMs' quantization performance, and store these sensitive weights in high precision. For example, AWQ stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates a per -channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR employs the L2 error between the original and quantized predictions as a weight sensitivity metric. Furthermore, SqueezeLLM introduces a weights clusters algorithm based on sensitivity , using k-means centroids as quantized weight values, to identify sensitive weights. The sensitivity is approximated by the Hessian matrix of weights. Then, SqueezeLLM stores sensitive weights in an ef ficient sparse format, and quantize other weights. SqueezeLLM quantizes LLMs in 3-bit, and achieves a more than 2x speedup compared to the FP16 baseline. Weight-Activation Quantization Alongside works centered on weight-only quantization in LLMs, there is a plethora of research focusing primarily on weight-activation quantization in LLMs. For example, ZeroQuant is the first work to implement weight-activation quantization for LLMs, which uses group-wise quantization for weight and token-wise quantization for activations, and reduces the precision for weights and activations of LLMs to INT8. LLMs have outliers in activations, and the performance of LLMs declines a lot, if these activations with outliers are directly quantized. Recent works try to treat these outliers specially to reduce quantization errors in weight- activation quantization. For example, LLM.int8() stores these outlier feature dimensions into high-precision, and uses vector -wise quantization, which assigns separate normalization constants to each inner product within matrix multiplication, to quantize other features. LLM.int8() quantizes weights and activations of LLMs into 8- bit without any performance degradation. SmoothQuant designs a per -channel scaling transformation to smooths the activation outliers based on the discovery that dif ferent tokens have similar variations across channels of activations. RPTQ finds that the range of values varies greatly between dif ferent channels, and integrates a channel reordering method, which clusters and reorders the channels in the activation and uses the",
    "context_length_chars": 9286,
    "rag_answer_length_words": 0,
    "no_rag_answer_length_words": 66,
    "retrieval_time_ms": 23.060868000357004,
    "rag_generation_time_ms": 8453.492417999769,
    "no_rag_generation_time_ms": 8147.043945000405,
    "total_rag_time_ms": 8476.553286000126
  },
  {
    "question_id": 9,
    "question": "What are the three categories of Post-Training Quantization?",
    "ground_truth": "PTQ for LLMs is categorized into three groups: (1) Weight-Only Quantization - focuses solely on quantizing weights, (2) Weight-Activation Quantization - extends to both weights and activations, and (3) KV Cache Quantization - targets the key-value cache in attention layers.",
    "rag_answer": "The three categories of Post-Training Quantization are: 1. Direct Use of Quantized Models in Inference (PTQ) 2. Quantization-Aware Training (QAT) 3.",
    "no_rag_answer": "There are several methods for quantizing models after training, but three common categories include: 1. Linear quantization: This method involves quantizing the weights and activations of a model using linear transformations such as rounding or truncation. It is often used in mobile devices or other resource-constrained environments where memory and computational resources are limited. Nonlinear quantization: This method involves applying nonlinear transformations to the weights and activations of a model to reduce their dynamic range and improve compression efficiency.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8441774982039121,
      0.7680011261280697,
      0.7749226096808499
    ],
    "avg_context_score": 0.7957004113376106,
    "full_context": "the two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization\n\nincludes quantization, pruning, knowledge distillation, and low-rank factorization. Furthermore, our study sheds light on prevailing challenges and of fers a glimpse into potential future research trajectories in this evolving field. We advocate for collaborative ef forts within the community to pave the way for an ecologically conscious, all-encompassing, and sustainable future for LLMs. While there were previous surveys on neural networks model compression and it has been lightly discussed in prior surveys on LMs and LLMs, our work is the inaugural survey dedicated solely to model compression for LLMs. Metrics and Benchmarks Metrics Model compression of LLMs can be measured using various metrics, which capture dif ferent aspects of performance. These metrics are commonly presented alongside accuracy and zero-shot ability to comprehensively evaluate the LLM. Model Size in a LLM typically is measured by the number of total parameters of the LLM. In general, LLMs with more parameters often requires more computational resources and memory for both training and inference. Floating Point Operations (FLOPs) is an indicator that measures the computational ef ficiency of LLMs, representing the number of floating-point operations required for the LLM to perform an instance. In model compression, reducing FLOPs helps to make the LLM run faster and more ef ficiently . Mean FLOPS Utilization (MFU) quantifies the practical ef ficiency of computational resource utilization by LLMs during tasks. MFU measures the ratio of actual FLOPS utilized by the LLM to the maximum theoretical FLOPS of a device. Unlike FLOPs, which estimates the maximum operations an LLM might perform, MFU assesses the actual ef fectiveness of resource use in operation. Essentially , while FLOPs measures a LLM's theoretical compute needs, MFU shows how ef fectively these computations are utilized in practice. Infer ence time (i.e., latency) measures the time taken by the LLM to process and generate responses for input data during inference. Inference time is particularly crucial for real-world applications where the LLM needs to respond for user queries or process lar ge amounts of data in real-time. Speedup Ratio measures how much faster a compressed LLM performs tasks compared to the original LLM. Specifically , it measures the ratio of the inference time of the uncompressed model over the inference time of the compressed model. Higher ratios mean greater ef ficiency and reduced computation time, highlighting effective compression. Compr ession Ratio measures how much a LLM's size is reduced through compression, calculated as the original size divided by the compressed size. Higher ratios mean greater size reduction, showing the compression's ef fectiveness in saving storage and memory .\n\nWeight-only quantization is the most conventional and widespread method. For example, LUT -GEMM uses binary-coding quantization (BCQ) format, which factorizes the parameters of LLMs into binary parameters and a set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ proposes a layer -wise quantization method based on Optimal Brain Quantization (OBQ), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from a calibration set, and multiplies weight and Hessian matrices with a Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important ef fect on LLMs' quantization performance, and store these sensitive weights in high precision. For example, AWQ stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates a per -channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR employs the L2 error between the original and quantized predictions as a weight sensitivity metric. Furthermore, SqueezeLLM introduces a weights clusters algorithm based on sensitivity , using k-means centroids as quantized weight values, to identify sensitive weights. The sensitivity is approximated by the Hessian matrix of weights. Then, SqueezeLLM stores sensitive weights in an ef ficient sparse format, and quantize other weights. SqueezeLLM quantizes LLMs in 3-bit, and achieves a more than 2x speedup compared to the FP16 baseline. Weight-Activation Quantization Alongside works centered on weight-only quantization in LLMs, there is a plethora of research focusing primarily on weight-activation quantization in LLMs. For example, ZeroQuant is the first work to implement weight-activation quantization for LLMs, which uses group-wise quantization for weight and token-wise quantization for activations, and reduces the precision for weights and activations of LLMs to INT8. LLMs have outliers in activations, and the performance of LLMs declines a lot, if these activations with outliers are directly quantized. Recent works try to treat these outliers specially to reduce quantization errors in weight- activation quantization. For example, LLM.int8() stores these outlier feature dimensions into high-precision, and uses vector -wise quantization, which assigns separate normalization constants to each inner product within matrix multiplication, to quantize other features. LLM.int8() quantizes weights and activations of LLMs into 8- bit without any performance degradation. SmoothQuant designs a per -channel scaling transformation to smooths the activation outliers based on the discovery that dif ferent tokens have similar variations across channels of activations. RPTQ finds that the range of values varies greatly between dif ferent channels, and integrates a channel reordering method, which clusters and reorders the channels in the activation and uses the",
    "context_length_chars": 9549,
    "rag_answer_length_words": 21,
    "no_rag_answer_length_words": 79,
    "retrieval_time_ms": 25.14668999992864,
    "rag_generation_time_ms": 3829.284073999588,
    "no_rag_generation_time_ms": 8157.569713000157,
    "total_rag_time_ms": 3854.430763999517
  },
  {
    "question_id": 10,
    "question": "What is GPTQ and how does it work?",
    "ground_truth": "GPTQ is a layer-wise quantization method based on Optimal Brain Quantization (OBQ) that updates weights with inverse Hessian information and quantizes LLMs into 3/4-bit precision.",
    "rag_answer": "GPTQ stands for Gradient-based Pruning and Quantization. It is a layer-wise quantization method based on Optimal Brain Quantization (OBQ).",
    "no_rag_answer": "GPTQ stands for \"Generative Pre-Training of Question Answering\" and is a type of natural language processing (NLP) technique used to train question answering models. It involves pre-training a model on a large corpus of text, such as Wikipedia or the web, using unsupervised learning algorithms. During the pre-training process, the model learns to generate coherent sequences of words based on the context in which they appear in the training data. This allows the model to understand the meaning of words and their relationships with other words in a sentence.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.7555197293022218,
      0.6992786584540269,
      0.7245481555176454
    ],
    "avg_context_score": 0.7264488477579647,
    "full_context": "Weight-only quantization is the most conventional and widespread method. For example, LUT -GEMM uses binary-coding quantization (BCQ) format, which factorizes the parameters of LLMs into binary parameters and a set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ proposes a layer -wise quantization method based on Optimal Brain Quantization (OBQ), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from a calibration set, and multiplies weight and Hessian matrices with a Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important ef fect on LLMs' quantization performance, and store these sensitive weights in high precision. For example, AWQ stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates a per -channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR employs the L2 error between the original and quantized predictions as a weight sensitivity metric. Furthermore, SqueezeLLM introduces a weights clusters algorithm based on sensitivity , using k-means centroids as quantized weight values, to identify sensitive weights. The sensitivity is approximated by the Hessian matrix of weights. Then, SqueezeLLM stores sensitive weights in an ef ficient sparse format, and quantize other weights. SqueezeLLM quantizes LLMs in 3-bit, and achieves a more than 2x speedup compared to the FP16 baseline. Weight-Activation Quantization Alongside works centered on weight-only quantization in LLMs, there is a plethora of research focusing primarily on weight-activation quantization in LLMs. For example, ZeroQuant is the first work to implement weight-activation quantization for LLMs, which uses group-wise quantization for weight and token-wise quantization for activations, and reduces the precision for weights and activations of LLMs to INT8. LLMs have outliers in activations, and the performance of LLMs declines a lot, if these activations with outliers are directly quantized. Recent works try to treat these outliers specially to reduce quantization errors in weight- activation quantization. For example, LLM.int8() stores these outlier feature dimensions into high-precision, and uses vector -wise quantization, which assigns separate normalization constants to each inner product within matrix multiplication, to quantize other features. LLM.int8() quantizes weights and activations of LLMs into 8- bit without any performance degradation. SmoothQuant designs a per -channel scaling transformation to smooths the activation outliers based on the discovery that dif ferent tokens have similar variations across channels of activations. RPTQ finds that the range of values varies greatly between dif ferent channels, and integrates a channel reordering method, which clusters and reorders the channels in the activation and uses the\n\nUnstructur ed Pruning Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular , necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT , which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver . SparseGPT achieves significant unstructured sparsity , even up to over 50% on the lar gest GPT models like OPT -175B and BLOOM-176B, with minimal increase in perplexity . To reduce the cost about the weight update process required by SparseGPT , Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity . Furthermore, DSnoT minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buf fers for computation using tensor cores. Structur ed Pruning Compared to unstructured pruning, structured pruning of fers the advantage of being hardware-agnostic, allowing for accelerated inference on traditional hardware post-pruning. However , the removal of lar ger and potentially more critical components in structured pruning may result in performance degradation, typically requiring ef ficient parameter fine-tuning for recovery . We divide LLMs structured pruning works into several groups based on pruning metrics: Loss-based Pruning, Magnitude-based Pruning, Regularization-based Pruning. Loss-based Pruning assesses the significance of a pruning unit by measuring its impact on loss or gradient information (e.g., first-order or second-order derivatives of loss). For example, LLM-Pruner introduces a one- shot structured pruning on LLMs based on gradient information. Specifically , LLM-Pruner identifies dependent structures via a dependency detection algorithm and selects optimal pruning groups using gradient information, rather than solely relying on loss changes, in a task-agnostic manner . Different from LLM-Pruner , which focuses on narrowing LLMs' width, Shortened LLaMA introduces a one-shot depth pruning on LLMs. Shortened LLaMA chooses the Transformer block as the prunable unit, and prunes these unimportant Transformer blocks, where the importance of Transformer blocks is evaluated by loss and its second-order\n\nthe two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization",
    "context_length_chars": 10037,
    "rag_answer_length_words": 19,
    "no_rag_answer_length_words": 89,
    "retrieval_time_ms": 24.21260099981737,
    "rag_generation_time_ms": 4320.81102400025,
    "no_rag_generation_time_ms": 8145.458561999476,
    "total_rag_time_ms": 4345.023625000067
  },
  {
    "question_id": 11,
    "question": "What is AWQ's approach to weight quantization?",
    "ground_truth": "AWQ (Activation-aware Weight Quantization) stores the top 1% of weights that have the most significant impact on LLM performance in high-precision and integrates a per-channel scaling method to identify optimal scaling factors.",
    "rag_answer": "AWQ's approach to weight quantization involves identifying sensitive weights, which are those that have a significant impact on the performance of the LLM, and storing them in high precision.",
    "no_rag_answer": "AWQ (Adaptive Weight Quantization) is a technique used in deep learning models for reducing the precision of weights and activations. The goal of this technique is to reduce the memory requirements and computational complexity of the model without significantly affecting its performance. AWQ uses an adaptive quantization strategy, which means that it adjusts the level of quantization based on the importance of each weight or activation in the model. This is done by analyzing the distribution of values in each layer and selecting the appropriate number of bits to represent them.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8942217239194559,
      0.8715065188639457,
      0.8443850612714456
    ],
    "avg_context_score": 0.8700377680182824,
    "full_context": "Weight-only quantization is the most conventional and widespread method. For example, LUT -GEMM uses binary-coding quantization (BCQ) format, which factorizes the parameters of LLMs into binary parameters and a set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ proposes a layer -wise quantization method based on Optimal Brain Quantization (OBQ), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from a calibration set, and multiplies weight and Hessian matrices with a Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important ef fect on LLMs' quantization performance, and store these sensitive weights in high precision. For example, AWQ stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates a per -channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR employs the L2 error between the original and quantized predictions as a weight sensitivity metric. Furthermore, SqueezeLLM introduces a weights clusters algorithm based on sensitivity , using k-means centroids as quantized weight values, to identify sensitive weights. The sensitivity is approximated by the Hessian matrix of weights. Then, SqueezeLLM stores sensitive weights in an ef ficient sparse format, and quantize other weights. SqueezeLLM quantizes LLMs in 3-bit, and achieves a more than 2x speedup compared to the FP16 baseline. Weight-Activation Quantization Alongside works centered on weight-only quantization in LLMs, there is a plethora of research focusing primarily on weight-activation quantization in LLMs. For example, ZeroQuant is the first work to implement weight-activation quantization for LLMs, which uses group-wise quantization for weight and token-wise quantization for activations, and reduces the precision for weights and activations of LLMs to INT8. LLMs have outliers in activations, and the performance of LLMs declines a lot, if these activations with outliers are directly quantized. Recent works try to treat these outliers specially to reduce quantization errors in weight- activation quantization. For example, LLM.int8() stores these outlier feature dimensions into high-precision, and uses vector -wise quantization, which assigns separate normalization constants to each inner product within matrix multiplication, to quantize other features. LLM.int8() quantizes weights and activations of LLMs into 8- bit without any performance degradation. SmoothQuant designs a per -channel scaling transformation to smooths the activation outliers based on the discovery that dif ferent tokens have similar variations across channels of activations. RPTQ finds that the range of values varies greatly between dif ferent channels, and integrates a channel reordering method, which clusters and reorders the channels in the activation and uses the\n\nthe two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization\n\nsame quantization parameters to quantize the values in each cluster , into layer normalization and linear layer weights to ef ficiently reduce the ef fect of numerical range dif ferences between channels. OliV e thinks that outliers are more important than the normal values, and uses an outlier -victim pair (OVP) quantization to handle outlier values locally with low hardware overheads and significant performance benefits. OS+ further finds that outliers are concentrated in specific and asymmetric channels. Based on the findings, OS+ incorporates channel- wise shifting to eliminate the impact of asymmetry and channel-wise scaling to balance the distribution of outliers. LLM-FP4 uses floating-point formats (specifically FP8 and FP4) to address the limitations of traditional integer quantization (such as INT8 and INT4) to deal with outliers. Furthermore, LLM-FP4 points out that exponent bits and clipping range are important factors that ef fect the performance of FP quantization, and introduces a search-based framework for determining the optimal exponent bias and maximal quantization value. OmniQuant handles the activation outliers by equivalently shifting the challenge of quantization from activations to weights, and optimizes the clipping threshold to adjust the extreme values of the weights. KV Cache Quantization With the increasing number of input tokens supported by LLMs, the memory usage of the KV cache also increases. Recent ef forts begin to focus on kv cache quantization to reduce the memory footprint of LLMs and accelerate their inference. For example, KVQuant proposes several KV Cache Quantization methods, such as Per-Channel Key Quantization, PreRoPE Key Quantization, and Non-Uniform kv cache quantization, to implement 10 million context length LLM inference. Through an in-depth analysis of the element distribution within the KV cache, KIVI finds that key caches should be quantized per -channel, while value caches should be quantized per -token. Finally , KIVI succeeds in quantizing the KV cache to 2 bits without fine-tuning. WKVQuant presents an innovative approach for quantizing lar ge language models (LLMs) by integrating past- only quantization to refine attention computations, employing a two-dimensional quantization strategy to manage the distribution of key/value (KV) caches ef fectively , and utilizing cross-block reconstruction regularization for optimizing parameters. This method enables the quantization of both weights and KV caches, resulting in memory savings that rival those of weight-activation quantization, while nearly matching the performance levels of weight-only quantization. Pruning Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant components. Pruning can be divided into Unstructured Pruning, Semi-Structured Pruning, and Structured Pruning. Structured pruning removes entire components like neurons, attention heads, or layers based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. Semi-structured pruning is a method that lies between structured pruning and unstructured pruning, capable of achieving fine-grained pruning and structural regularization simultaneously . It prunes partial parameters based on specific patterns rather than entire channels, filters, or neurons, making it a fine-grained form of structured pruning.",
    "context_length_chars": 10243,
    "rag_answer_length_words": 29,
    "no_rag_answer_length_words": 91,
    "retrieval_time_ms": 24.806168999930378,
    "rag_generation_time_ms": 4711.107824999999,
    "no_rag_generation_time_ms": 8147.086599999966,
    "total_rag_time_ms": 4735.9139939999295
  },
  {
    "question_id": 12,
    "question": "Why do LLMs have issues with activation quantization?",
    "ground_truth": "LLMs have outliers in activations, and the performance declines significantly if these activations with outliers are directly quantized. Recent works treat these outliers specially to reduce quantization errors.",
    "rag_answer": "LLMs have issues with activation quantization because it can lead to a decrease in their accuracy during inference. This is because quantizing activations reduces the dynamic range of the input data, leading to potential overflow or underflow of the intermediate results, which can cause rounding errors and affect the overall performance of the model. Additionally, quantizing activations may also introduce bias into the model, which can further degrade its accuracy.",
    "no_rag_answer": "Activation quantization is a technique used to reduce the precision of activations in neural networks during training or inference. This can help improve computational efficiency and reduce memory usage, which can be beneficial for large models or real-time applications. However, there are some potential issues that can arise when using activation quantization: 1. Loss of accuracy: Reducing the precision of activations can lead to a loss of accuracy in the model's predictions.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.863362718525643,
      0.8834026035615334,
      0.8657569677695207
    ],
    "avg_context_score": 0.8708407632855657,
    "full_context": "Weight-only quantization is the most conventional and widespread method. For example, LUT -GEMM uses binary-coding quantization (BCQ) format, which factorizes the parameters of LLMs into binary parameters and a set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ proposes a layer -wise quantization method based on Optimal Brain Quantization (OBQ), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from a calibration set, and multiplies weight and Hessian matrices with a Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important ef fect on LLMs' quantization performance, and store these sensitive weights in high precision. For example, AWQ stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates a per -channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR employs the L2 error between the original and quantized predictions as a weight sensitivity metric. Furthermore, SqueezeLLM introduces a weights clusters algorithm based on sensitivity , using k-means centroids as quantized weight values, to identify sensitive weights. The sensitivity is approximated by the Hessian matrix of weights. Then, SqueezeLLM stores sensitive weights in an ef ficient sparse format, and quantize other weights. SqueezeLLM quantizes LLMs in 3-bit, and achieves a more than 2x speedup compared to the FP16 baseline. Weight-Activation Quantization Alongside works centered on weight-only quantization in LLMs, there is a plethora of research focusing primarily on weight-activation quantization in LLMs. For example, ZeroQuant is the first work to implement weight-activation quantization for LLMs, which uses group-wise quantization for weight and token-wise quantization for activations, and reduces the precision for weights and activations of LLMs to INT8. LLMs have outliers in activations, and the performance of LLMs declines a lot, if these activations with outliers are directly quantized. Recent works try to treat these outliers specially to reduce quantization errors in weight- activation quantization. For example, LLM.int8() stores these outlier feature dimensions into high-precision, and uses vector -wise quantization, which assigns separate normalization constants to each inner product within matrix multiplication, to quantize other features. LLM.int8() quantizes weights and activations of LLMs into 8- bit without any performance degradation. SmoothQuant designs a per -channel scaling transformation to smooths the activation outliers based on the discovery that dif ferent tokens have similar variations across channels of activations. RPTQ finds that the range of values varies greatly between dif ferent channels, and integrates a channel reordering method, which clusters and reorders the channels in the activation and uses the\n\nthe two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization\n\nsame quantization parameters to quantize the values in each cluster , into layer normalization and linear layer weights to ef ficiently reduce the ef fect of numerical range dif ferences between channels. OliV e thinks that outliers are more important than the normal values, and uses an outlier -victim pair (OVP) quantization to handle outlier values locally with low hardware overheads and significant performance benefits. OS+ further finds that outliers are concentrated in specific and asymmetric channels. Based on the findings, OS+ incorporates channel- wise shifting to eliminate the impact of asymmetry and channel-wise scaling to balance the distribution of outliers. LLM-FP4 uses floating-point formats (specifically FP8 and FP4) to address the limitations of traditional integer quantization (such as INT8 and INT4) to deal with outliers. Furthermore, LLM-FP4 points out that exponent bits and clipping range are important factors that ef fect the performance of FP quantization, and introduces a search-based framework for determining the optimal exponent bias and maximal quantization value. OmniQuant handles the activation outliers by equivalently shifting the challenge of quantization from activations to weights, and optimizes the clipping threshold to adjust the extreme values of the weights. KV Cache Quantization With the increasing number of input tokens supported by LLMs, the memory usage of the KV cache also increases. Recent ef forts begin to focus on kv cache quantization to reduce the memory footprint of LLMs and accelerate their inference. For example, KVQuant proposes several KV Cache Quantization methods, such as Per-Channel Key Quantization, PreRoPE Key Quantization, and Non-Uniform kv cache quantization, to implement 10 million context length LLM inference. Through an in-depth analysis of the element distribution within the KV cache, KIVI finds that key caches should be quantized per -channel, while value caches should be quantized per -token. Finally , KIVI succeeds in quantizing the KV cache to 2 bits without fine-tuning. WKVQuant presents an innovative approach for quantizing lar ge language models (LLMs) by integrating past- only quantization to refine attention computations, employing a two-dimensional quantization strategy to manage the distribution of key/value (KV) caches ef fectively , and utilizing cross-block reconstruction regularization for optimizing parameters. This method enables the quantization of both weights and KV caches, resulting in memory savings that rival those of weight-activation quantization, while nearly matching the performance levels of weight-only quantization. Pruning Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant components. Pruning can be divided into Unstructured Pruning, Semi-Structured Pruning, and Structured Pruning. Structured pruning removes entire components like neurons, attention heads, or layers based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. Semi-structured pruning is a method that lies between structured pruning and unstructured pruning, capable of achieving fine-grained pruning and structural regularization simultaneously . It prunes partial parameters based on specific patterns rather than entire channels, filters, or neurons, making it a fine-grained form of structured pruning.",
    "context_length_chars": 10243,
    "rag_answer_length_words": 70,
    "no_rag_answer_length_words": 72,
    "retrieval_time_ms": 24.387450000176614,
    "rag_generation_time_ms": 8160.492689000421,
    "no_rag_generation_time_ms": 8133.990516000267,
    "total_rag_time_ms": 8184.880139000597
  },
  {
    "question_id": 13,
    "question": "What is SmoothQuant's approach?",
    "ground_truth": "SmoothQuant designs a per-channel scaling transformation to smooth the activation outliers based on the discovery that different tokens have similar variations across channels of activations.",
    "rag_answer": "SmoothQuant's approach involves using a combination of methods such as layer normalization, linear layer weights, and outlier-victim pair (OVP) quantization to efficiently reduce the effect of numerical range differences between channels. It also incorporates channel-wise shifting and scaling to eliminate the impact of asymmetry and balance the distribution of outliers.",
    "no_rag_answer": "SmoothQuant is a quantitative trading platform that uses machine learning algorithms to analyze market data and generate trading signals. The company's approach involves using historical price data, technical indicators, and other market information to train its models and develop predictive models for future price movements. SmoothQuant's models are designed to be scalable and can handle large amounts of data, making them suitable for use in high-frequency trading environments. The platform also offers a range of customization options, allowing traders to fine-tune the models to suit their specific needs and risk profiles.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.622763334693472,
      0.6094744077533374,
      0.5763098807219009
    ],
    "avg_context_score": 0.6028492077229034,
    "full_context": "methods, metrics, and benchmark datasets. By diving into LLM compression, we've highlighted its challenges and opportunities. This survey aims to be a valuable reference, providing insights into the current landscape and promoting ongoing exploration of this pivotal topic. Acknowledgments We would like to thank the anonymous reviewers and the Action Editor for their valuable feedback and discussions. The work of Jian Li is supported partially by National Natural Science Foundation of China . The work of Yong Liu is supported partially by National Natural Science Foundation of China , Beijing Outstanding Young Scientist Program , the Unicom Innovation Ecological Cooperation Plan, and the CCF-Huawei Populus Grove Fund.\n\nsame quantization parameters to quantize the values in each cluster , into layer normalization and linear layer weights to ef ficiently reduce the ef fect of numerical range dif ferences between channels. OliV e thinks that outliers are more important than the normal values, and uses an outlier -victim pair (OVP) quantization to handle outlier values locally with low hardware overheads and significant performance benefits. OS+ further finds that outliers are concentrated in specific and asymmetric channels. Based on the findings, OS+ incorporates channel- wise shifting to eliminate the impact of asymmetry and channel-wise scaling to balance the distribution of outliers. LLM-FP4 uses floating-point formats (specifically FP8 and FP4) to address the limitations of traditional integer quantization (such as INT8 and INT4) to deal with outliers. Furthermore, LLM-FP4 points out that exponent bits and clipping range are important factors that ef fect the performance of FP quantization, and introduces a search-based framework for determining the optimal exponent bias and maximal quantization value. OmniQuant handles the activation outliers by equivalently shifting the challenge of quantization from activations to weights, and optimizes the clipping threshold to adjust the extreme values of the weights. KV Cache Quantization With the increasing number of input tokens supported by LLMs, the memory usage of the KV cache also increases. Recent ef forts begin to focus on kv cache quantization to reduce the memory footprint of LLMs and accelerate their inference. For example, KVQuant proposes several KV Cache Quantization methods, such as Per-Channel Key Quantization, PreRoPE Key Quantization, and Non-Uniform kv cache quantization, to implement 10 million context length LLM inference. Through an in-depth analysis of the element distribution within the KV cache, KIVI finds that key caches should be quantized per -channel, while value caches should be quantized per -token. Finally , KIVI succeeds in quantizing the KV cache to 2 bits without fine-tuning. WKVQuant presents an innovative approach for quantizing lar ge language models (LLMs) by integrating past- only quantization to refine attention computations, employing a two-dimensional quantization strategy to manage the distribution of key/value (KV) caches ef fectively , and utilizing cross-block reconstruction regularization for optimizing parameters. This method enables the quantization of both weights and KV caches, resulting in memory savings that rival those of weight-activation quantization, while nearly matching the performance levels of weight-only quantization. Pruning Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant components. Pruning can be divided into Unstructured Pruning, Semi-Structured Pruning, and Structured Pruning. Structured pruning removes entire components like neurons, attention heads, or layers based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. Semi-structured pruning is a method that lies between structured pruning and unstructured pruning, capable of achieving fine-grained pruning and structural regularization simultaneously . It prunes partial parameters based on specific patterns rather than entire channels, filters, or neurons, making it a fine-grained form of structured pruning.\n\nthe two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization",
    "context_length_chars": 7396,
    "rag_answer_length_words": 50,
    "no_rag_answer_length_words": 91,
    "retrieval_time_ms": 23.91777600041678,
    "rag_generation_time_ms": 13576.374947000659,
    "no_rag_generation_time_ms": 8138.09651199972,
    "total_rag_time_ms": 13600.292723001076
  },
  {
    "question_id": 14,
    "question": "What is KV cache quantization and why is it important?",
    "ground_truth": "KV cache quantization targets the KV cache, which stores keys and values of attention layers. It's important because the KV cache often consumes lots of memory and acts as a bottleneck for input streams containing lengthy tokens. By implementing KV cache quantization, it's possible to increase throughput and accommodate inputs with longer tokens more efficiently.",
    "rag_answer": "KV cache quantization refers to the process of reducing the memory footprint of knowledge vectors (KV) cache used in large language models (LLMs) by quantizing its elements. This is important because as the number of input tokens supported by LLMs increases, so does the memory usage of the KV cache, which can slow down or even crash the model's inference.",
    "no_rag_answer": "Key-Value (KV) cache quantization refers to the process of reducing the precision or accuracy of values stored in a KV cache. This technique is used to reduce memory usage, improve performance, and optimize resource utilization in distributed systems that use caching. In traditional KV cache implementations, each value is stored as a floating-point number or a string. However, these data types can consume significant amounts of memory, especially when dealing with large datasets.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8507314281298193,
      0.831374452182386,
      0.8271861042038022
    ],
    "avg_context_score": 0.8364306615053358,
    "full_context": "same quantization parameters to quantize the values in each cluster , into layer normalization and linear layer weights to ef ficiently reduce the ef fect of numerical range dif ferences between channels. OliV e thinks that outliers are more important than the normal values, and uses an outlier -victim pair (OVP) quantization to handle outlier values locally with low hardware overheads and significant performance benefits. OS+ further finds that outliers are concentrated in specific and asymmetric channels. Based on the findings, OS+ incorporates channel- wise shifting to eliminate the impact of asymmetry and channel-wise scaling to balance the distribution of outliers. LLM-FP4 uses floating-point formats (specifically FP8 and FP4) to address the limitations of traditional integer quantization (such as INT8 and INT4) to deal with outliers. Furthermore, LLM-FP4 points out that exponent bits and clipping range are important factors that ef fect the performance of FP quantization, and introduces a search-based framework for determining the optimal exponent bias and maximal quantization value. OmniQuant handles the activation outliers by equivalently shifting the challenge of quantization from activations to weights, and optimizes the clipping threshold to adjust the extreme values of the weights. KV Cache Quantization With the increasing number of input tokens supported by LLMs, the memory usage of the KV cache also increases. Recent ef forts begin to focus on kv cache quantization to reduce the memory footprint of LLMs and accelerate their inference. For example, KVQuant proposes several KV Cache Quantization methods, such as Per-Channel Key Quantization, PreRoPE Key Quantization, and Non-Uniform kv cache quantization, to implement 10 million context length LLM inference. Through an in-depth analysis of the element distribution within the KV cache, KIVI finds that key caches should be quantized per -channel, while value caches should be quantized per -token. Finally , KIVI succeeds in quantizing the KV cache to 2 bits without fine-tuning. WKVQuant presents an innovative approach for quantizing lar ge language models (LLMs) by integrating past- only quantization to refine attention computations, employing a two-dimensional quantization strategy to manage the distribution of key/value (KV) caches ef fectively , and utilizing cross-block reconstruction regularization for optimizing parameters. This method enables the quantization of both weights and KV caches, resulting in memory savings that rival those of weight-activation quantization, while nearly matching the performance levels of weight-only quantization. Pruning Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant components. Pruning can be divided into Unstructured Pruning, Semi-Structured Pruning, and Structured Pruning. Structured pruning removes entire components like neurons, attention heads, or layers based on specific rules while preserving the overall network structure. On the other hand, unstructured pruning prunes individual parameters, resulting in an irregular sparse structure. Semi-structured pruning is a method that lies between structured pruning and unstructured pruning, capable of achieving fine-grained pruning and structural regularization simultaneously . It prunes partial parameters based on specific patterns rather than entire channels, filters, or neurons, making it a fine-grained form of structured pruning.\n\nthe two approaches lies in whether retraining is needed during quantization. PTQ enables direct use of quantized models in inference, while QA T requires retraining to rectify errors introduced by quantization. Quantization-A ware Training QAT involves retraining a quantized model to counteract performance degradation caused by quantization. For instance, LLM-QA T implements the standard QA T framework directly onto LLMs. LLM-QA T distills knowledge by generating data from the LLM itself, and train the quantized LLM to align with the output distribution of the original LLM based on the generated data. BitDistiller mer ges QA T with self-distillation, enhancing LLM performance at sub-4-bit precisions. It employs tailored asymmetric quantization, clipping, and a Confidence-A ware Kullback-Leibler Diver gence objective for faster conver gence and superior results. OneBit introduces a novel 1-bit parameter representation method and an ef fective parameter initialization method to implement 1-bit quantization for LLM weight matrices, paving the way for the extremely low bit-width deployment of LLMs. Remark: While QA T can mitigate quantization's accuracy degradation, retraining demands a lot of ef fort due to tens or hundreds of billions of parameters in LLMs. A practical solution is to incorporate Parameter -Efficient Fine-T uning (PEFT) into the retraining process of QA T. Currently , methods like QLORA, PEQA and LoftQ combine quantization with PEFT for model fine-tuning ef ficiency . However , these methods are typically task- dependent. L4Q makes a preliminary attempt to enhance generality by leveraging LoRA-wise learned quantization step size for LLMs. We think that introducing PEFT to enhance QA T efficiency is not only feasible but also holds significant promise, warranting thorough exploration. Post-T raining Quantization PTQ ef ficiently converts a full-precision LLM to low-precision without retraining, saving memory and computational costs. We categorize PTQ for LLMs into three groups: Weight-Only Quantization, Weight- Activation Quantization, and KV Cache Quantization. The disparity between these groups lies in their quantization objectives. Weight-only quantization focuses solely on quantizing weights, whereas weight- activation quantization extends its objective to both weights and activations. Prior research indicates that activation quantization is typically more sensitive to weight quantization, allowing weight-only quantization to achieve lower bit-width. However , since quantized weights necessitate dequantization before multiplication with activations, weight-only quantization inevitably introduces additional computational overhead during inference and cannot enjoy the accelerated low-bit operation supported by specific hardware. Furthermore, kv cache quantization tar gets the KV cache, which stores keys and values of attention layers. The KV cache often consumes lots of memory , acting as a bottleneck for input streams containing lengthy tokens. By implementing kv cache quantization, it is possible to increase throughput and accommodate inputs with longer tokens more efficiently . Weight-Only Quantization\n\nWeight-only quantization is the most conventional and widespread method. For example, LUT -GEMM uses binary-coding quantization (BCQ) format, which factorizes the parameters of LLMs into binary parameters and a set of scaling factors, to accelerate quantized matrix multiplications in weight-only quantization. GPTQ proposes a layer -wise quantization method based on Optimal Brain Quantization (OBQ), which updates weights with inverse Hessian information, and quantizes LLMs into 3/4-bit. QuIP optimally adjusts weights by utilizing the LDL decomposition of the Hessian matrix derived from vectors drawn uniformly at random from a calibration set, and multiplies weight and Hessian matrices with a Kronecker product of random orthogonal matrices to ensure incoherence between weight and Hessian matrices. Combining these two steps, QuIP successfully quantizes LLMs into 2-bits with minimal performance loss. To further minimize quantization errors in the weight-only quantization of LLMs, lots of works identify sensitive weights, which have an important ef fect on LLMs' quantization performance, and store these sensitive weights in high precision. For example, AWQ stores the top 1% of weights that have the most significant impact on LLM performance in high-precision, and integrates a per -channel scaling method to identify optimal scaling factors. Here, \"channel\" denotes individual dimensions or feature maps within the model. Similar with AWQ, OWQ store weights sensitive to activation outliers in high-precision, and quantizes other non-sensitive weights. Different from OWQ, SpQR employs the L2 error between the original and quantized predictions as a weight sensitivity metric. Furthermore, SqueezeLLM introduces a weights clusters algorithm based on sensitivity , using k-means centroids as quantized weight values, to identify sensitive weights. The sensitivity is approximated by the Hessian matrix of weights. Then, SqueezeLLM stores sensitive weights in an ef ficient sparse format, and quantize other weights. SqueezeLLM quantizes LLMs in 3-bit, and achieves a more than 2x speedup compared to the FP16 baseline. Weight-Activation Quantization Alongside works centered on weight-only quantization in LLMs, there is a plethora of research focusing primarily on weight-activation quantization in LLMs. For example, ZeroQuant is the first work to implement weight-activation quantization for LLMs, which uses group-wise quantization for weight and token-wise quantization for activations, and reduces the precision for weights and activations of LLMs to INT8. LLMs have outliers in activations, and the performance of LLMs declines a lot, if these activations with outliers are directly quantized. Recent works try to treat these outliers specially to reduce quantization errors in weight- activation quantization. For example, LLM.int8() stores these outlier feature dimensions into high-precision, and uses vector -wise quantization, which assigns separate normalization constants to each inner product within matrix multiplication, to quantize other features. LLM.int8() quantizes weights and activations of LLMs into 8- bit without any performance degradation. SmoothQuant designs a per -channel scaling transformation to smooths the activation outliers based on the discovery that dif ferent tokens have similar variations across channels of activations. RPTQ finds that the range of values varies greatly between dif ferent channels, and integrates a channel reordering method, which clusters and reorders the channels in the activation and uses the",
    "context_length_chars": 10243,
    "rag_answer_length_words": 60,
    "no_rag_answer_length_words": 73,
    "retrieval_time_ms": 23.103994999473798,
    "rag_generation_time_ms": 15406.880338999144,
    "no_rag_generation_time_ms": 8138.045925999904,
    "total_rag_time_ms": 15429.984333998618
  },
  {
    "question_id": 15,
    "question": "What is pruning in neural networks?",
    "ground_truth": "Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant components such as individual parameters, neurons, attention heads, or entire layers.",
    "rag_answer": "Pruning in neural networks refers to the process of removing unnecessary or redundant connections between neurons during training, in order to reduce the size of the model and improve its efficiency. This is typically done by assigning importance scores to each connection and selectively setting some of them to zero, based on these scores. Structured pruning involves dividing the weight matrix into groups and selecting a subset of weights within each group based on their importance scores. Different pruning strategies may be used depending on the target hardware and the specific requirements of the task at hand.",
    "no_rag_answer": "Pruning in neural networks refers to the process of removing unnecessary or redundant connections between neurons, typically during the training process. This technique is used to reduce the complexity of the model and prevent overfitting, which can improve its generalization performance on new data. Pruning can be performed at different levels, such as at the connection level (removing individual synapses), the layer level (removing entire layers), or the subnetwork level (removing entire subnetworks).",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8749625356740278,
      0.8910155568921638,
      0.8633491329829255
    ],
    "avg_context_score": 0.8764424085163723,
    "full_context": "lowest reconstruction error(based on Hessian information), ensuring a sparsity ratio of N:M. This process iteratively prunes and updates model weights, addressing one block at a time until the desired sparsity level is achieved across the entire model. Wanda achieves structured N:M pruning by dividing the weight matrix into groups of M consecutive weights and computing an importance score for each weight. The score is determined by the product of the weight's magnitude and the norm of the corresponding input activations. Within each weight group, the N weights with the highest scores are retained, while the rest are set to zero, thereby implementing structured N:M pruning. Furthermore, choosing the optimal pruning strategy is crucial for compatibility with the tar get hardware. For instance, the Ampere Tensor Core GPU architecture (e.g., A100 GPUs) proposes 2:4 fine-grained semi-structured sparsity to accelerate Sparse Neural Networks on this hardware. However , the current implementation of the Ampere architecture supports only the 2:4 ratio, leaving other ratios without acceleration. Remark: LLMs often perform well on multiple tasks, which means they contain a multitude of parameters for various tasks. Dynamic pruning methods can dynamically prune dif ferent parts of the model based on the current task's requirements to provide better performance on specific tasks. This helps strike a balance between performance and ef ficiency . Remark: For PTQ and pruning, preparing a high-quality calibration dataset to assist in improving the performance of compressed LLMs is crucial. Specifically , extensive empirical studies on the ef fect of calibration data upon model compression methods find that the performance of downstream tasks can vary significantly depending on the calibration data selected. High-quality calibration data can improve the performance and accuracy of the compressed model, so careful selection and preparation of calibration data are necessary . Knowledge Distillation Knowledge Distillation (KD) is a technique aimed at transferring knowledge from a lar ge and complex model (i.e., teacher model) to a smaller and simpler model (i.e., student model). We classify these methods into two clear categories: Black-box KD, where only the teacher's outputs are accessible, typically from closed-source LLMs, and White-box KD, where the teacher's parameters or output distribution are available, usually from open-source LLMs. Black-box KD Black-box KD usually prompts the teacher LLM to generate a distillation dataset for fine-tune the student LM, thereby transfering capabilities from teacher LLM to the student LM. In Black-box KD, teacher LLMs such as ChatGPT (gpt-3.5-turbo) and GPT4 are typically employed, while smaller LMs (SLMs), such as GPT -2, T5, FlanT5, and CodeT5, are commonly utilized as student LMs. On the other hand, researchers find that LLMs have emer gent abilities, which refers to a significant improvement in performance when the model reaches a certain scale, showcasing surprising capabilities. Lots of Black-box KD methods try to distill emer gent abilities\n\nderivative. After Pruning, both LLM-Pruner and Shortened LLaMA utilize LoRA to rapidly recover the performance of the pruned model. Magnitude-based Pruning involves devising a heuristic metric based on the magnitudes of pruning units, and use the metric to assess the importance of pruning units, subsequently pruning those units whose scores fall below a predefined threshold. For example, FLAP utilizes a structured fluctuation metric to assess and identify columns in the weight matrix suitable for pruning, measuring the variation of each input feature relative to a baseline value to estimate the impact of removing a column of weights. Additionally , FLAP uses an adaptive structure search to optimize global model compression, and restores the model's performance post-pruning through a baseline bias compensation mechanism, avoiding the need for fine-tuning. To further maintain the pruned model's performance, SliceGPT leverages the computational invariance of transformer networks and optimizes the pruning process through Principal Component Analysis (PCA). Specifically , SliceGPT employs PCA as the pruning metric, applying it at each layer of the transformer network to project the signal matrix onto its principal components and eliminate insignificant columns or rows from the transformed weight matrices, ultimately aiming to compress the model ef fectively . Regularization-based Pruning typically adds a regularization term (e.g., L0, L1, and L2 regularization) into the loss function to induce sparsity for LLMs. For example, Sheared LLaMA uses a pair of Lagrange multipliers based on pruning masks to impose constraints on the pruned model shape directly , thereby formulating pruning as a constrained optimization problem. Through solving this optimization problem, Sheared LLaMA derives optimal pruning masks. Additionally , Sheared LLaMA introduces dynamic batch loading, a strategy that adapts training data loading based on each domain's loss reduction rate, enhancing the ef ficiency of data utilization during training. Remark: Structured pruning typically reduces model size by removing redundant parameters, but it may degrade model performance. A novel approach is to combine knowledge distillation with structured pruning. Knowledge distillation allows knowledge extracted from a LLM to be transferred to a smaller model, helping the smaller model maintain its performance while reducing its size. Semi-Structur ed Pruning Apart from unstructured pruning and structured pruning, there are many works which use semi-structured pruning to prune partial weights of LLMs based on specific patterns. N:M sparsity , where every M contiguous elements leave N non-zero elements, is an example of semi-structured pruning. For example, E-Sparse implements N:M sparsity by introducing information entropy as a metric for evaluating parameter importance to enhances the significance of parameter weights and input feature norms. E-Sparse incorporates global naive shuffle and local block shuf fle to ef ficiently optimize information distribution and mitigate the impact of N:M sparsity on LLM accuracy . Furthermore, many pruning works can also be generalized to semi-structured patterns. For example, SparseGPT and Wanda also explore N:M sparsity of LLMs. SparseGPT employs block- wise weight partitioning, with each block containing M weights. It identifies and prunes N weights with the\n\nUnstructur ed Pruning Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular , necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT , which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver . SparseGPT achieves significant unstructured sparsity , even up to over 50% on the lar gest GPT models like OPT -175B and BLOOM-176B, with minimal increase in perplexity . To reduce the cost about the weight update process required by SparseGPT , Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity . Furthermore, DSnoT minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buf fers for computation using tensor cores. Structur ed Pruning Compared to unstructured pruning, structured pruning of fers the advantage of being hardware-agnostic, allowing for accelerated inference on traditional hardware post-pruning. However , the removal of lar ger and potentially more critical components in structured pruning may result in performance degradation, typically requiring ef ficient parameter fine-tuning for recovery . We divide LLMs structured pruning works into several groups based on pruning metrics: Loss-based Pruning, Magnitude-based Pruning, Regularization-based Pruning. Loss-based Pruning assesses the significance of a pruning unit by measuring its impact on loss or gradient information (e.g., first-order or second-order derivatives of loss). For example, LLM-Pruner introduces a one- shot structured pruning on LLMs based on gradient information. Specifically , LLM-Pruner identifies dependent structures via a dependency detection algorithm and selects optimal pruning groups using gradient information, rather than solely relying on loss changes, in a task-agnostic manner . Different from LLM-Pruner , which focuses on narrowing LLMs' width, Shortened LLaMA introduces a one-shot depth pruning on LLMs. Shortened LLaMA chooses the Transformer block as the prunable unit, and prunes these unimportant Transformer blocks, where the importance of Transformer blocks is evaluated by loss and its second-order",
    "context_length_chars": 9822,
    "rag_answer_length_words": 97,
    "no_rag_answer_length_words": 73,
    "retrieval_time_ms": 23.538618999737082,
    "rag_generation_time_ms": 8686.469975999898,
    "no_rag_generation_time_ms": 8142.165651000141,
    "total_rag_time_ms": 8710.008594999636
  },
  {
    "question_id": 16,
    "question": "What are the three types of pruning methods?",
    "ground_truth": "The three types are: (1) Unstructured Pruning - prunes individual parameters resulting in irregular sparse structure, (2) Structured Pruning - removes entire components like neurons or layers while preserving network structure, and (3) Semi-Structured Pruning - lies between the two, achieving fine-grained pruning with structural regularization.",
    "rag_answer": "Magnitude-based Pruning 2. Regularization-based Pruning 3.",
    "no_rag_answer": "Pruning is a technique used to improve the accuracy and performance of machine learning models by removing unnecessary or redundant connections between neurons. There are several pruning methods, but here are three common ones: 1. Magnitude-based pruning: This method involves setting the weights of the least important connections in the model to zero. The importance of each connection is determined based on its magnitude or absolute value.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8738406364104492,
      0.8298123377712727,
      0.7844634241129995
    ],
    "avg_context_score": 0.8293721327649072,
    "full_context": "derivative. After Pruning, both LLM-Pruner and Shortened LLaMA utilize LoRA to rapidly recover the performance of the pruned model. Magnitude-based Pruning involves devising a heuristic metric based on the magnitudes of pruning units, and use the metric to assess the importance of pruning units, subsequently pruning those units whose scores fall below a predefined threshold. For example, FLAP utilizes a structured fluctuation metric to assess and identify columns in the weight matrix suitable for pruning, measuring the variation of each input feature relative to a baseline value to estimate the impact of removing a column of weights. Additionally , FLAP uses an adaptive structure search to optimize global model compression, and restores the model's performance post-pruning through a baseline bias compensation mechanism, avoiding the need for fine-tuning. To further maintain the pruned model's performance, SliceGPT leverages the computational invariance of transformer networks and optimizes the pruning process through Principal Component Analysis (PCA). Specifically , SliceGPT employs PCA as the pruning metric, applying it at each layer of the transformer network to project the signal matrix onto its principal components and eliminate insignificant columns or rows from the transformed weight matrices, ultimately aiming to compress the model ef fectively . Regularization-based Pruning typically adds a regularization term (e.g., L0, L1, and L2 regularization) into the loss function to induce sparsity for LLMs. For example, Sheared LLaMA uses a pair of Lagrange multipliers based on pruning masks to impose constraints on the pruned model shape directly , thereby formulating pruning as a constrained optimization problem. Through solving this optimization problem, Sheared LLaMA derives optimal pruning masks. Additionally , Sheared LLaMA introduces dynamic batch loading, a strategy that adapts training data loading based on each domain's loss reduction rate, enhancing the ef ficiency of data utilization during training. Remark: Structured pruning typically reduces model size by removing redundant parameters, but it may degrade model performance. A novel approach is to combine knowledge distillation with structured pruning. Knowledge distillation allows knowledge extracted from a LLM to be transferred to a smaller model, helping the smaller model maintain its performance while reducing its size. Semi-Structur ed Pruning Apart from unstructured pruning and structured pruning, there are many works which use semi-structured pruning to prune partial weights of LLMs based on specific patterns. N:M sparsity , where every M contiguous elements leave N non-zero elements, is an example of semi-structured pruning. For example, E-Sparse implements N:M sparsity by introducing information entropy as a metric for evaluating parameter importance to enhances the significance of parameter weights and input feature norms. E-Sparse incorporates global naive shuffle and local block shuf fle to ef ficiently optimize information distribution and mitigate the impact of N:M sparsity on LLM accuracy . Furthermore, many pruning works can also be generalized to semi-structured patterns. For example, SparseGPT and Wanda also explore N:M sparsity of LLMs. SparseGPT employs block- wise weight partitioning, with each block containing M weights. It identifies and prunes N weights with the\n\nUnstructur ed Pruning Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular , necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT , which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver . SparseGPT achieves significant unstructured sparsity , even up to over 50% on the lar gest GPT models like OPT -175B and BLOOM-176B, with minimal increase in perplexity . To reduce the cost about the weight update process required by SparseGPT , Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity . Furthermore, DSnoT minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buf fers for computation using tensor cores. Structur ed Pruning Compared to unstructured pruning, structured pruning of fers the advantage of being hardware-agnostic, allowing for accelerated inference on traditional hardware post-pruning. However , the removal of lar ger and potentially more critical components in structured pruning may result in performance degradation, typically requiring ef ficient parameter fine-tuning for recovery . We divide LLMs structured pruning works into several groups based on pruning metrics: Loss-based Pruning, Magnitude-based Pruning, Regularization-based Pruning. Loss-based Pruning assesses the significance of a pruning unit by measuring its impact on loss or gradient information (e.g., first-order or second-order derivatives of loss). For example, LLM-Pruner introduces a one- shot structured pruning on LLMs based on gradient information. Specifically , LLM-Pruner identifies dependent structures via a dependency detection algorithm and selects optimal pruning groups using gradient information, rather than solely relying on loss changes, in a task-agnostic manner . Different from LLM-Pruner , which focuses on narrowing LLMs' width, Shortened LLaMA introduces a one-shot depth pruning on LLMs. Shortened LLaMA chooses the Transformer block as the prunable unit, and prunes these unimportant Transformer blocks, where the importance of Transformer blocks is evaluated by loss and its second-order\n\nlowest reconstruction error(based on Hessian information), ensuring a sparsity ratio of N:M. This process iteratively prunes and updates model weights, addressing one block at a time until the desired sparsity level is achieved across the entire model. Wanda achieves structured N:M pruning by dividing the weight matrix into groups of M consecutive weights and computing an importance score for each weight. The score is determined by the product of the weight's magnitude and the norm of the corresponding input activations. Within each weight group, the N weights with the highest scores are retained, while the rest are set to zero, thereby implementing structured N:M pruning. Furthermore, choosing the optimal pruning strategy is crucial for compatibility with the tar get hardware. For instance, the Ampere Tensor Core GPU architecture (e.g., A100 GPUs) proposes 2:4 fine-grained semi-structured sparsity to accelerate Sparse Neural Networks on this hardware. However , the current implementation of the Ampere architecture supports only the 2:4 ratio, leaving other ratios without acceleration. Remark: LLMs often perform well on multiple tasks, which means they contain a multitude of parameters for various tasks. Dynamic pruning methods can dynamically prune dif ferent parts of the model based on the current task's requirements to provide better performance on specific tasks. This helps strike a balance between performance and ef ficiency . Remark: For PTQ and pruning, preparing a high-quality calibration dataset to assist in improving the performance of compressed LLMs is crucial. Specifically , extensive empirical studies on the ef fect of calibration data upon model compression methods find that the performance of downstream tasks can vary significantly depending on the calibration data selected. High-quality calibration data can improve the performance and accuracy of the compressed model, so careful selection and preparation of calibration data are necessary . Knowledge Distillation Knowledge Distillation (KD) is a technique aimed at transferring knowledge from a lar ge and complex model (i.e., teacher model) to a smaller and simpler model (i.e., student model). We classify these methods into two clear categories: Black-box KD, where only the teacher's outputs are accessible, typically from closed-source LLMs, and White-box KD, where the teacher's parameters or output distribution are available, usually from open-source LLMs. Black-box KD Black-box KD usually prompts the teacher LLM to generate a distillation dataset for fine-tune the student LM, thereby transfering capabilities from teacher LLM to the student LM. In Black-box KD, teacher LLMs such as ChatGPT (gpt-3.5-turbo) and GPT4 are typically employed, while smaller LMs (SLMs), such as GPT -2, T5, FlanT5, and CodeT5, are commonly utilized as student LMs. On the other hand, researchers find that LLMs have emer gent abilities, which refers to a significant improvement in performance when the model reaches a certain scale, showcasing surprising capabilities. Lots of Black-box KD methods try to distill emer gent abilities",
    "context_length_chars": 9822,
    "rag_answer_length_words": 6,
    "no_rag_answer_length_words": 67,
    "retrieval_time_ms": 24.114855999869178,
    "rag_generation_time_ms": 10931.911101000878,
    "no_rag_generation_time_ms": 8141.355210999791,
    "total_rag_time_ms": 10956.025957000747
  },
  {
    "question_id": 17,
    "question": "What is SparseGPT?",
    "ground_truth": "SparseGPT is a one-shot pruning strategy without retraining that frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. It achieves significant unstructured sparsity, even up to over 50% on models like OPT-175B and BLOOM-176B, with minimal increase in perplexity.",
    "rag_answer": "SparseGPT is an innovative approach in the domain of unstructured pruning of LLMs that introduces a one-shot pruning strategy without retraining. It frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver.",
    "no_rag_answer": "SparseGPT is a variant of the language model GPT (Generative Pre-Trained Transformer) that has been optimized for sparse input sequences. In traditional GPT models, each token in an input sequence is represented as a dense vector, which can be computationally expensive and memory-intensive when dealing with long sequences or large vocabularies. SparseGPT addresses this issue by using a technique called \"sparsification\" to represent each token as a sparse vector instead of a dense one.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8975081667235809,
      0.7862217199004746,
      0.7820586144905235
    ],
    "avg_context_score": 0.8219295003715263,
    "full_context": "Unstructur ed Pruning Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular , necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT , which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver . SparseGPT achieves significant unstructured sparsity , even up to over 50% on the lar gest GPT models like OPT -175B and BLOOM-176B, with minimal increase in perplexity . To reduce the cost about the weight update process required by SparseGPT , Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity . Furthermore, DSnoT minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buf fers for computation using tensor cores. Structur ed Pruning Compared to unstructured pruning, structured pruning of fers the advantage of being hardware-agnostic, allowing for accelerated inference on traditional hardware post-pruning. However , the removal of lar ger and potentially more critical components in structured pruning may result in performance degradation, typically requiring ef ficient parameter fine-tuning for recovery . We divide LLMs structured pruning works into several groups based on pruning metrics: Loss-based Pruning, Magnitude-based Pruning, Regularization-based Pruning. Loss-based Pruning assesses the significance of a pruning unit by measuring its impact on loss or gradient information (e.g., first-order or second-order derivatives of loss). For example, LLM-Pruner introduces a one- shot structured pruning on LLMs based on gradient information. Specifically , LLM-Pruner identifies dependent structures via a dependency detection algorithm and selects optimal pruning groups using gradient information, rather than solely relying on loss changes, in a task-agnostic manner . Different from LLM-Pruner , which focuses on narrowing LLMs' width, Shortened LLaMA introduces a one-shot depth pruning on LLMs. Shortened LLaMA chooses the Transformer block as the prunable unit, and prunes these unimportant Transformer blocks, where the importance of Transformer blocks is evaluated by loss and its second-order\n\nderivative. After Pruning, both LLM-Pruner and Shortened LLaMA utilize LoRA to rapidly recover the performance of the pruned model. Magnitude-based Pruning involves devising a heuristic metric based on the magnitudes of pruning units, and use the metric to assess the importance of pruning units, subsequently pruning those units whose scores fall below a predefined threshold. For example, FLAP utilizes a structured fluctuation metric to assess and identify columns in the weight matrix suitable for pruning, measuring the variation of each input feature relative to a baseline value to estimate the impact of removing a column of weights. Additionally , FLAP uses an adaptive structure search to optimize global model compression, and restores the model's performance post-pruning through a baseline bias compensation mechanism, avoiding the need for fine-tuning. To further maintain the pruned model's performance, SliceGPT leverages the computational invariance of transformer networks and optimizes the pruning process through Principal Component Analysis (PCA). Specifically , SliceGPT employs PCA as the pruning metric, applying it at each layer of the transformer network to project the signal matrix onto its principal components and eliminate insignificant columns or rows from the transformed weight matrices, ultimately aiming to compress the model ef fectively . Regularization-based Pruning typically adds a regularization term (e.g., L0, L1, and L2 regularization) into the loss function to induce sparsity for LLMs. For example, Sheared LLaMA uses a pair of Lagrange multipliers based on pruning masks to impose constraints on the pruned model shape directly , thereby formulating pruning as a constrained optimization problem. Through solving this optimization problem, Sheared LLaMA derives optimal pruning masks. Additionally , Sheared LLaMA introduces dynamic batch loading, a strategy that adapts training data loading based on each domain's loss reduction rate, enhancing the ef ficiency of data utilization during training. Remark: Structured pruning typically reduces model size by removing redundant parameters, but it may degrade model performance. A novel approach is to combine knowledge distillation with structured pruning. Knowledge distillation allows knowledge extracted from a LLM to be transferred to a smaller model, helping the smaller model maintain its performance while reducing its size. Semi-Structur ed Pruning Apart from unstructured pruning and structured pruning, there are many works which use semi-structured pruning to prune partial weights of LLMs based on specific patterns. N:M sparsity , where every M contiguous elements leave N non-zero elements, is an example of semi-structured pruning. For example, E-Sparse implements N:M sparsity by introducing information entropy as a metric for evaluating parameter importance to enhances the significance of parameter weights and input feature norms. E-Sparse incorporates global naive shuffle and local block shuf fle to ef ficiently optimize information distribution and mitigate the impact of N:M sparsity on LLM accuracy . Furthermore, many pruning works can also be generalized to semi-structured patterns. For example, SparseGPT and Wanda also explore N:M sparsity of LLMs. SparseGPT employs block- wise weight partitioning, with each block containing M weights. It identifies and prunes N weights with the\n\nlowest reconstruction error(based on Hessian information), ensuring a sparsity ratio of N:M. This process iteratively prunes and updates model weights, addressing one block at a time until the desired sparsity level is achieved across the entire model. Wanda achieves structured N:M pruning by dividing the weight matrix into groups of M consecutive weights and computing an importance score for each weight. The score is determined by the product of the weight's magnitude and the norm of the corresponding input activations. Within each weight group, the N weights with the highest scores are retained, while the rest are set to zero, thereby implementing structured N:M pruning. Furthermore, choosing the optimal pruning strategy is crucial for compatibility with the tar get hardware. For instance, the Ampere Tensor Core GPU architecture (e.g., A100 GPUs) proposes 2:4 fine-grained semi-structured sparsity to accelerate Sparse Neural Networks on this hardware. However , the current implementation of the Ampere architecture supports only the 2:4 ratio, leaving other ratios without acceleration. Remark: LLMs often perform well on multiple tasks, which means they contain a multitude of parameters for various tasks. Dynamic pruning methods can dynamically prune dif ferent parts of the model based on the current task's requirements to provide better performance on specific tasks. This helps strike a balance between performance and ef ficiency . Remark: For PTQ and pruning, preparing a high-quality calibration dataset to assist in improving the performance of compressed LLMs is crucial. Specifically , extensive empirical studies on the ef fect of calibration data upon model compression methods find that the performance of downstream tasks can vary significantly depending on the calibration data selected. High-quality calibration data can improve the performance and accuracy of the compressed model, so careful selection and preparation of calibration data are necessary . Knowledge Distillation Knowledge Distillation (KD) is a technique aimed at transferring knowledge from a lar ge and complex model (i.e., teacher model) to a smaller and simpler model (i.e., student model). We classify these methods into two clear categories: Black-box KD, where only the teacher's outputs are accessible, typically from closed-source LLMs, and White-box KD, where the teacher's parameters or output distribution are available, usually from open-source LLMs. Black-box KD Black-box KD usually prompts the teacher LLM to generate a distillation dataset for fine-tune the student LM, thereby transfering capabilities from teacher LLM to the student LM. In Black-box KD, teacher LLMs such as ChatGPT (gpt-3.5-turbo) and GPT4 are typically employed, while smaller LMs (SLMs), such as GPT -2, T5, FlanT5, and CodeT5, are commonly utilized as student LMs. On the other hand, researchers find that LLMs have emer gent abilities, which refers to a significant improvement in performance when the model reaches a certain scale, showcasing surprising capabilities. Lots of Black-box KD methods try to distill emer gent abilities",
    "context_length_chars": 9822,
    "rag_answer_length_words": 39,
    "no_rag_answer_length_words": 74,
    "retrieval_time_ms": 24.702880999939225,
    "rag_generation_time_ms": 15264.593101999708,
    "no_rag_generation_time_ms": 8113.366423999651,
    "total_rag_time_ms": 15289.295982999647
  },
  {
    "question_id": 18,
    "question": "What is Wanda's pruning approach?",
    "ground_truth": "Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates.",
    "rag_answer": "",
    "no_rag_answer": "Wanda, a large language model trained by Mistral AI, uses a technique called \"pruning\" to reduce the size of its model and improve its efficiency. Pruning involves removing unnecessary connections between neurons in the model, which can significantly reduce the number of parameters that need to be stored and processed during inference. Wanda's pruning approach is based on a combination of magnitude-based pruning and movement-based pruning. In magnitude-based pruning, the model identifies low-magnitude weights and removes them from the network.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.888796834282159,
      0.8002349385421876,
      0.7908446302082535
    ],
    "avg_context_score": 0.8266254676775334,
    "full_context": "Unstructur ed Pruning Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular , necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT , which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver . SparseGPT achieves significant unstructured sparsity , even up to over 50% on the lar gest GPT models like OPT -175B and BLOOM-176B, with minimal increase in perplexity . To reduce the cost about the weight update process required by SparseGPT , Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity . Furthermore, DSnoT minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buf fers for computation using tensor cores. Structur ed Pruning Compared to unstructured pruning, structured pruning of fers the advantage of being hardware-agnostic, allowing for accelerated inference on traditional hardware post-pruning. However , the removal of lar ger and potentially more critical components in structured pruning may result in performance degradation, typically requiring ef ficient parameter fine-tuning for recovery . We divide LLMs structured pruning works into several groups based on pruning metrics: Loss-based Pruning, Magnitude-based Pruning, Regularization-based Pruning. Loss-based Pruning assesses the significance of a pruning unit by measuring its impact on loss or gradient information (e.g., first-order or second-order derivatives of loss). For example, LLM-Pruner introduces a one- shot structured pruning on LLMs based on gradient information. Specifically , LLM-Pruner identifies dependent structures via a dependency detection algorithm and selects optimal pruning groups using gradient information, rather than solely relying on loss changes, in a task-agnostic manner . Different from LLM-Pruner , which focuses on narrowing LLMs' width, Shortened LLaMA introduces a one-shot depth pruning on LLMs. Shortened LLaMA chooses the Transformer block as the prunable unit, and prunes these unimportant Transformer blocks, where the importance of Transformer blocks is evaluated by loss and its second-order\n\nderivative. After Pruning, both LLM-Pruner and Shortened LLaMA utilize LoRA to rapidly recover the performance of the pruned model. Magnitude-based Pruning involves devising a heuristic metric based on the magnitudes of pruning units, and use the metric to assess the importance of pruning units, subsequently pruning those units whose scores fall below a predefined threshold. For example, FLAP utilizes a structured fluctuation metric to assess and identify columns in the weight matrix suitable for pruning, measuring the variation of each input feature relative to a baseline value to estimate the impact of removing a column of weights. Additionally , FLAP uses an adaptive structure search to optimize global model compression, and restores the model's performance post-pruning through a baseline bias compensation mechanism, avoiding the need for fine-tuning. To further maintain the pruned model's performance, SliceGPT leverages the computational invariance of transformer networks and optimizes the pruning process through Principal Component Analysis (PCA). Specifically , SliceGPT employs PCA as the pruning metric, applying it at each layer of the transformer network to project the signal matrix onto its principal components and eliminate insignificant columns or rows from the transformed weight matrices, ultimately aiming to compress the model ef fectively . Regularization-based Pruning typically adds a regularization term (e.g., L0, L1, and L2 regularization) into the loss function to induce sparsity for LLMs. For example, Sheared LLaMA uses a pair of Lagrange multipliers based on pruning masks to impose constraints on the pruned model shape directly , thereby formulating pruning as a constrained optimization problem. Through solving this optimization problem, Sheared LLaMA derives optimal pruning masks. Additionally , Sheared LLaMA introduces dynamic batch loading, a strategy that adapts training data loading based on each domain's loss reduction rate, enhancing the ef ficiency of data utilization during training. Remark: Structured pruning typically reduces model size by removing redundant parameters, but it may degrade model performance. A novel approach is to combine knowledge distillation with structured pruning. Knowledge distillation allows knowledge extracted from a LLM to be transferred to a smaller model, helping the smaller model maintain its performance while reducing its size. Semi-Structur ed Pruning Apart from unstructured pruning and structured pruning, there are many works which use semi-structured pruning to prune partial weights of LLMs based on specific patterns. N:M sparsity , where every M contiguous elements leave N non-zero elements, is an example of semi-structured pruning. For example, E-Sparse implements N:M sparsity by introducing information entropy as a metric for evaluating parameter importance to enhances the significance of parameter weights and input feature norms. E-Sparse incorporates global naive shuffle and local block shuf fle to ef ficiently optimize information distribution and mitigate the impact of N:M sparsity on LLM accuracy . Furthermore, many pruning works can also be generalized to semi-structured patterns. For example, SparseGPT and Wanda also explore N:M sparsity of LLMs. SparseGPT employs block- wise weight partitioning, with each block containing M weights. It identifies and prunes N weights with the\n\nlowest reconstruction error(based on Hessian information), ensuring a sparsity ratio of N:M. This process iteratively prunes and updates model weights, addressing one block at a time until the desired sparsity level is achieved across the entire model. Wanda achieves structured N:M pruning by dividing the weight matrix into groups of M consecutive weights and computing an importance score for each weight. The score is determined by the product of the weight's magnitude and the norm of the corresponding input activations. Within each weight group, the N weights with the highest scores are retained, while the rest are set to zero, thereby implementing structured N:M pruning. Furthermore, choosing the optimal pruning strategy is crucial for compatibility with the tar get hardware. For instance, the Ampere Tensor Core GPU architecture (e.g., A100 GPUs) proposes 2:4 fine-grained semi-structured sparsity to accelerate Sparse Neural Networks on this hardware. However , the current implementation of the Ampere architecture supports only the 2:4 ratio, leaving other ratios without acceleration. Remark: LLMs often perform well on multiple tasks, which means they contain a multitude of parameters for various tasks. Dynamic pruning methods can dynamically prune dif ferent parts of the model based on the current task's requirements to provide better performance on specific tasks. This helps strike a balance between performance and ef ficiency . Remark: For PTQ and pruning, preparing a high-quality calibration dataset to assist in improving the performance of compressed LLMs is crucial. Specifically , extensive empirical studies on the ef fect of calibration data upon model compression methods find that the performance of downstream tasks can vary significantly depending on the calibration data selected. High-quality calibration data can improve the performance and accuracy of the compressed model, so careful selection and preparation of calibration data are necessary . Knowledge Distillation Knowledge Distillation (KD) is a technique aimed at transferring knowledge from a lar ge and complex model (i.e., teacher model) to a smaller and simpler model (i.e., student model). We classify these methods into two clear categories: Black-box KD, where only the teacher's outputs are accessible, typically from closed-source LLMs, and White-box KD, where the teacher's parameters or output distribution are available, usually from open-source LLMs. Black-box KD Black-box KD usually prompts the teacher LLM to generate a distillation dataset for fine-tune the student LM, thereby transfering capabilities from teacher LLM to the student LM. In Black-box KD, teacher LLMs such as ChatGPT (gpt-3.5-turbo) and GPT4 are typically employed, while smaller LMs (SLMs), such as GPT -2, T5, FlanT5, and CodeT5, are commonly utilized as student LMs. On the other hand, researchers find that LLMs have emer gent abilities, which refers to a significant improvement in performance when the model reaches a certain scale, showcasing surprising capabilities. Lots of Black-box KD methods try to distill emer gent abilities",
    "context_length_chars": 9822,
    "rag_answer_length_words": 0,
    "no_rag_answer_length_words": 80,
    "retrieval_time_ms": 23.94843199999741,
    "rag_generation_time_ms": 6000.380794999728,
    "no_rag_generation_time_ms": 8134.759016999851,
    "total_rag_time_ms": 6024.3292269997255
  },
  {
    "question_id": 19,
    "question": "What are the three categories of structured pruning metrics?",
    "ground_truth": "Structured pruning works are divided into: (1) Loss-based Pruning - assesses significance by measuring impact on loss or gradient information, (2) Magnitude-based Pruning - uses heuristic metrics based on magnitudes of pruning units, and (3) Regularization-based Pruning - adds regularization terms to the loss function to induce sparsity.",
    "rag_answer": "Structured fluctuation metric used by FLAP to assess and identify columns in the weight matrix suitable for pruning. Principal Component Analysis (PCA) used by SliceGPT to optimize the pruning process.",
    "no_rag_answer": "Structured pruning metrics can be categorized into three main groups: 1. **Pruning Efficiency**: This category measures how efficiently a pruning algorithm is able to remove unnecessary connections from a model, typically in terms of computational cost, memory usage or time complexity. Metrics in this group include the number of operations saved, the amount of memory reduced, and the time taken to perform pruning. **Model Quality**: This category assesses the impact of pruning on the quality of the resulting model, including its accuracy, performance, and ability to generalize to new data.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8940944237195065,
      0.8267512796407956,
      0.8125876131957739
    ],
    "avg_context_score": 0.8444777721853587,
    "full_context": "derivative. After Pruning, both LLM-Pruner and Shortened LLaMA utilize LoRA to rapidly recover the performance of the pruned model. Magnitude-based Pruning involves devising a heuristic metric based on the magnitudes of pruning units, and use the metric to assess the importance of pruning units, subsequently pruning those units whose scores fall below a predefined threshold. For example, FLAP utilizes a structured fluctuation metric to assess and identify columns in the weight matrix suitable for pruning, measuring the variation of each input feature relative to a baseline value to estimate the impact of removing a column of weights. Additionally , FLAP uses an adaptive structure search to optimize global model compression, and restores the model's performance post-pruning through a baseline bias compensation mechanism, avoiding the need for fine-tuning. To further maintain the pruned model's performance, SliceGPT leverages the computational invariance of transformer networks and optimizes the pruning process through Principal Component Analysis (PCA). Specifically , SliceGPT employs PCA as the pruning metric, applying it at each layer of the transformer network to project the signal matrix onto its principal components and eliminate insignificant columns or rows from the transformed weight matrices, ultimately aiming to compress the model ef fectively . Regularization-based Pruning typically adds a regularization term (e.g., L0, L1, and L2 regularization) into the loss function to induce sparsity for LLMs. For example, Sheared LLaMA uses a pair of Lagrange multipliers based on pruning masks to impose constraints on the pruned model shape directly , thereby formulating pruning as a constrained optimization problem. Through solving this optimization problem, Sheared LLaMA derives optimal pruning masks. Additionally , Sheared LLaMA introduces dynamic batch loading, a strategy that adapts training data loading based on each domain's loss reduction rate, enhancing the ef ficiency of data utilization during training. Remark: Structured pruning typically reduces model size by removing redundant parameters, but it may degrade model performance. A novel approach is to combine knowledge distillation with structured pruning. Knowledge distillation allows knowledge extracted from a LLM to be transferred to a smaller model, helping the smaller model maintain its performance while reducing its size. Semi-Structur ed Pruning Apart from unstructured pruning and structured pruning, there are many works which use semi-structured pruning to prune partial weights of LLMs based on specific patterns. N:M sparsity , where every M contiguous elements leave N non-zero elements, is an example of semi-structured pruning. For example, E-Sparse implements N:M sparsity by introducing information entropy as a metric for evaluating parameter importance to enhances the significance of parameter weights and input feature norms. E-Sparse incorporates global naive shuffle and local block shuf fle to ef ficiently optimize information distribution and mitigate the impact of N:M sparsity on LLM accuracy . Furthermore, many pruning works can also be generalized to semi-structured patterns. For example, SparseGPT and Wanda also explore N:M sparsity of LLMs. SparseGPT employs block- wise weight partitioning, with each block containing M weights. It identifies and prunes N weights with the\n\nlowest reconstruction error(based on Hessian information), ensuring a sparsity ratio of N:M. This process iteratively prunes and updates model weights, addressing one block at a time until the desired sparsity level is achieved across the entire model. Wanda achieves structured N:M pruning by dividing the weight matrix into groups of M consecutive weights and computing an importance score for each weight. The score is determined by the product of the weight's magnitude and the norm of the corresponding input activations. Within each weight group, the N weights with the highest scores are retained, while the rest are set to zero, thereby implementing structured N:M pruning. Furthermore, choosing the optimal pruning strategy is crucial for compatibility with the tar get hardware. For instance, the Ampere Tensor Core GPU architecture (e.g., A100 GPUs) proposes 2:4 fine-grained semi-structured sparsity to accelerate Sparse Neural Networks on this hardware. However , the current implementation of the Ampere architecture supports only the 2:4 ratio, leaving other ratios without acceleration. Remark: LLMs often perform well on multiple tasks, which means they contain a multitude of parameters for various tasks. Dynamic pruning methods can dynamically prune dif ferent parts of the model based on the current task's requirements to provide better performance on specific tasks. This helps strike a balance between performance and ef ficiency . Remark: For PTQ and pruning, preparing a high-quality calibration dataset to assist in improving the performance of compressed LLMs is crucial. Specifically , extensive empirical studies on the ef fect of calibration data upon model compression methods find that the performance of downstream tasks can vary significantly depending on the calibration data selected. High-quality calibration data can improve the performance and accuracy of the compressed model, so careful selection and preparation of calibration data are necessary . Knowledge Distillation Knowledge Distillation (KD) is a technique aimed at transferring knowledge from a lar ge and complex model (i.e., teacher model) to a smaller and simpler model (i.e., student model). We classify these methods into two clear categories: Black-box KD, where only the teacher's outputs are accessible, typically from closed-source LLMs, and White-box KD, where the teacher's parameters or output distribution are available, usually from open-source LLMs. Black-box KD Black-box KD usually prompts the teacher LLM to generate a distillation dataset for fine-tune the student LM, thereby transfering capabilities from teacher LLM to the student LM. In Black-box KD, teacher LLMs such as ChatGPT (gpt-3.5-turbo) and GPT4 are typically employed, while smaller LMs (SLMs), such as GPT -2, T5, FlanT5, and CodeT5, are commonly utilized as student LMs. On the other hand, researchers find that LLMs have emer gent abilities, which refers to a significant improvement in performance when the model reaches a certain scale, showcasing surprising capabilities. Lots of Black-box KD methods try to distill emer gent abilities\n\nUnstructur ed Pruning Unstructured pruning preserves the pruned model's performance, hence, works related to unstructured pruning of LLMs often dispense with retraining to restore performance. Nevertheless, unstructured pruning renders the pruned model irregular , necessitating specialized handling or software optimizations for inference acceleration. An innovative approach in this domain is SparseGPT , which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver . SparseGPT achieves significant unstructured sparsity , even up to over 50% on the lar gest GPT models like OPT -175B and BLOOM-176B, with minimal increase in perplexity . To reduce the cost about the weight update process required by SparseGPT , Wanda achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity . Furthermore, DSnoT minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels. To provide hardware support for handling unstructured pruning on the GPU Tensor Core hardware, Flash-LLM introduces an unstructured sparse matrix multiplication method, which loads weight matrices in a sparse format from global memory and reconstructs them in a dense format within high-speed on-chip buf fers for computation using tensor cores. Structur ed Pruning Compared to unstructured pruning, structured pruning of fers the advantage of being hardware-agnostic, allowing for accelerated inference on traditional hardware post-pruning. However , the removal of lar ger and potentially more critical components in structured pruning may result in performance degradation, typically requiring ef ficient parameter fine-tuning for recovery . We divide LLMs structured pruning works into several groups based on pruning metrics: Loss-based Pruning, Magnitude-based Pruning, Regularization-based Pruning. Loss-based Pruning assesses the significance of a pruning unit by measuring its impact on loss or gradient information (e.g., first-order or second-order derivatives of loss). For example, LLM-Pruner introduces a one- shot structured pruning on LLMs based on gradient information. Specifically , LLM-Pruner identifies dependent structures via a dependency detection algorithm and selects optimal pruning groups using gradient information, rather than solely relying on loss changes, in a task-agnostic manner . Different from LLM-Pruner , which focuses on narrowing LLMs' width, Shortened LLaMA introduces a one-shot depth pruning on LLMs. Shortened LLaMA chooses the Transformer block as the prunable unit, and prunes these unimportant Transformer blocks, where the importance of Transformer blocks is evaluated by loss and its second-order",
    "context_length_chars": 9822,
    "rag_answer_length_words": 30,
    "no_rag_answer_length_words": 90,
    "retrieval_time_ms": 23.872634999861475,
    "rag_generation_time_ms": 14584.488245999637,
    "no_rag_generation_time_ms": 8142.438668000068,
    "total_rag_time_ms": 14608.360880999498
  },
  {
    "question_id": 20,
    "question": "What is N:M sparsity?",
    "ground_truth": "N:M sparsity is a semi-structured pruning pattern where every M contiguous elements leave N non-zero elements. For example, 2:4 sparsity means that in every 4 consecutive elements, 2 are kept non-zero.",
    "rag_answer": "N:M sparsity refers to a method of pruning neural network models where the weight matrix is divided into groups of M consecutive weights and an importance score is computed for each weight based on its magnitude and the norm of the corresponding input activations.",
    "no_rag_answer": "In the context of matrices, \"N:M\" refers to a specific type of sparsity pattern. Sparsity in a matrix means that not all elements are non-zero (i.e., some elements are zero). The degree of sparsity can be quantified using various metrics such as the fraction of non-zero entries or the number of non-zero entries per row/column.",
    "num_chunks_retrieved": 3,
    "context_scores": [
      0.8078528388896018,
      0.782184325460717,
      0.7551721282362802
    ],
    "avg_context_score": 0.7817364308621997,
    "full_context": "lowest reconstruction error(based on Hessian information), ensuring a sparsity ratio of N:M. This process iteratively prunes and updates model weights, addressing one block at a time until the desired sparsity level is achieved across the entire model. Wanda achieves structured N:M pruning by dividing the weight matrix into groups of M consecutive weights and computing an importance score for each weight. The score is determined by the product of the weight's magnitude and the norm of the corresponding input activations. Within each weight group, the N weights with the highest scores are retained, while the rest are set to zero, thereby implementing structured N:M pruning. Furthermore, choosing the optimal pruning strategy is crucial for compatibility with the tar get hardware. For instance, the Ampere Tensor Core GPU architecture (e.g., A100 GPUs) proposes 2:4 fine-grained semi-structured sparsity to accelerate Sparse Neural Networks on this hardware. However , the current implementation of the Ampere architecture supports only the 2:4 ratio, leaving other ratios without acceleration. Remark: LLMs often perform well on multiple tasks, which means they contain a multitude of parameters for various tasks. Dynamic pruning methods can dynamically prune dif ferent parts of the model based on the current task's requirements to provide better performance on specific tasks. This helps strike a balance between performance and ef ficiency . Remark: For PTQ and pruning, preparing a high-quality calibration dataset to assist in improving the performance of compressed LLMs is crucial. Specifically , extensive empirical studies on the ef fect of calibration data upon model compression methods find that the performance of downstream tasks can vary significantly depending on the calibration data selected. High-quality calibration data can improve the performance and accuracy of the compressed model, so careful selection and preparation of calibration data are necessary . Knowledge Distillation Knowledge Distillation (KD) is a technique aimed at transferring knowledge from a lar ge and complex model (i.e., teacher model) to a smaller and simpler model (i.e., student model). We classify these methods into two clear categories: Black-box KD, where only the teacher's outputs are accessible, typically from closed-source LLMs, and White-box KD, where the teacher's parameters or output distribution are available, usually from open-source LLMs. Black-box KD Black-box KD usually prompts the teacher LLM to generate a distillation dataset for fine-tune the student LM, thereby transfering capabilities from teacher LLM to the student LM. In Black-box KD, teacher LLMs such as ChatGPT (gpt-3.5-turbo) and GPT4 are typically employed, while smaller LMs (SLMs), such as GPT -2, T5, FlanT5, and CodeT5, are commonly utilized as student LMs. On the other hand, researchers find that LLMs have emer gent abilities, which refers to a significant improvement in performance when the model reaches a certain scale, showcasing surprising capabilities. Lots of Black-box KD methods try to distill emer gent abilities\n\nLLM Infer ence and Deployment The ef ficiency of compressed LLMs during deployment is also a significant area for exploration. This involves multiple evaluation metrics, including arithmetic intensity , memory size, and throughput. Furthermore, we can use an analytical tool, the Roofline Model, to assess the resource ef ficiency of compressed LLMs on specific hardware. Evaluating the deployment ef ficiency of compressed LLMs on specific hardware can guide researchers in selecting and analyzing the advantages and disadvantages of various model compression methods and further optimizing these methods. The Effect of Scaling Law The scaling law underscores the significant impact of model size, dataset size, and compute resources on the performance of LLMs. However , the scaling law presents a fundamental challenge for LLM compression, i.e., there is a trade-of f between model size and performance in compressed LLMs. Delving into the mechanisms and theories underpinning the scaling law is crucial for elucidating and potentially overcoming this limitation. AutoML for LLM Compr ession Existing compression techniques have made remarkable progress, but they still heavily depend on manual design. For instance, designing appropriate student architectures for knowledge distillation requires a significant amount of human ef fort. To reduce this reliance on manual design, a feasible solution is to combine Automated Machine Learning (AutoML) techniques such as Meta-Learning and Neural Architecture Search (NAS) with model compression. By combining with AutoML techniques, model compression can automatically select appropriate hyperparameters and tailor architectures and scales of compressed models, thus minimizing human involvement and lowering the associated costs. Furthermore, AutoML can identify optimal model compression strategies tailored to specific task requirements, thereby further enhancing compression rates without compromising model performance. Explainability of LLM Compr ession Earlier research has raised significant concerns regarding the explainability of model compression techniques applied to Pre-trained Language Models (PLMs). Notably , these same challenges extend to LLM compression methods as well. For example, CoT -distillation can enhance SLMs' reasoning performance, yet the mechanism through which it imparts CoT ability remains unclear . This challenge underscores the importance of integrating explainability with model compression approaches for the advancement of LLM compression applications. Explainability not only clarifies the changes and trade-of fs in the compression process but also enhances efficiency and accuracy . Additionally , interpretability aids in evaluating the compressed model's performance to ensure it aligns with practical requirements. Conclusion In the survey , we have explored model compression techniques for LLMs. Our coverage spanned compression\n\nderivative. After Pruning, both LLM-Pruner and Shortened LLaMA utilize LoRA to rapidly recover the performance of the pruned model. Magnitude-based Pruning involves devising a heuristic metric based on the magnitudes of pruning units, and use the metric to assess the importance of pruning units, subsequently pruning those units whose scores fall below a predefined threshold. For example, FLAP utilizes a structured fluctuation metric to assess and identify columns in the weight matrix suitable for pruning, measuring the variation of each input feature relative to a baseline value to estimate the impact of removing a column of weights. Additionally , FLAP uses an adaptive structure search to optimize global model compression, and restores the model's performance post-pruning through a baseline bias compensation mechanism, avoiding the need for fine-tuning. To further maintain the pruned model's performance, SliceGPT leverages the computational invariance of transformer networks and optimizes the pruning process through Principal Component Analysis (PCA). Specifically , SliceGPT employs PCA as the pruning metric, applying it at each layer of the transformer network to project the signal matrix onto its principal components and eliminate insignificant columns or rows from the transformed weight matrices, ultimately aiming to compress the model ef fectively . Regularization-based Pruning typically adds a regularization term (e.g., L0, L1, and L2 regularization) into the loss function to induce sparsity for LLMs. For example, Sheared LLaMA uses a pair of Lagrange multipliers based on pruning masks to impose constraints on the pruned model shape directly , thereby formulating pruning as a constrained optimization problem. Through solving this optimization problem, Sheared LLaMA derives optimal pruning masks. Additionally , Sheared LLaMA introduces dynamic batch loading, a strategy that adapts training data loading based on each domain's loss reduction rate, enhancing the ef ficiency of data utilization during training. Remark: Structured pruning typically reduces model size by removing redundant parameters, but it may degrade model performance. A novel approach is to combine knowledge distillation with structured pruning. Knowledge distillation allows knowledge extracted from a LLM to be transferred to a smaller model, helping the smaller model maintain its performance while reducing its size. Semi-Structur ed Pruning Apart from unstructured pruning and structured pruning, there are many works which use semi-structured pruning to prune partial weights of LLMs based on specific patterns. N:M sparsity , where every M contiguous elements leave N non-zero elements, is an example of semi-structured pruning. For example, E-Sparse implements N:M sparsity by introducing information entropy as a metric for evaluating parameter importance to enhances the significance of parameter weights and input feature norms. E-Sparse incorporates global naive shuffle and local block shuf fle to ef ficiently optimize information distribution and mitigate the impact of N:M sparsity on LLM accuracy . Furthermore, many pruning works can also be generalized to semi-structured patterns. For example, SparseGPT and Wanda also explore N:M sparsity of LLMs. SparseGPT employs block- wise weight partitioning, with each block containing M weights. It identifies and prunes N weights with the",
    "context_length_chars": 9467,
    "rag_answer_length_words": 44,
    "no_rag_answer_length_words": 55,
    "retrieval_time_ms": 23.58960899982776,
    "rag_generation_time_ms": 5412.354177000452,
    "no_rag_generation_time_ms": 8134.495066999989,
    "total_rag_time_ms": 5435.94378600028
  }
]