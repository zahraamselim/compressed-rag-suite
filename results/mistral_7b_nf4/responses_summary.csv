id,question,ground_truth,rag_answer,no_rag_answer,chunks,avg_score,rag_words,no_rag_words
1,What is the main focus of this survey paper?,This paper presents a comprehensive survey of model compression techniques for Large Language Models...,The main focus of this survey paper is to provide insights into the current landscape and promote on...,"Without a specific survey paper to reference, I am unable to provide an answer to this question. Ple...",3,0.5771776338418325,64,38
2,What challenges do Large Language Models face?,LLMs face challenges stemming from their extensive size and computational requirements. For instance...,Large Language Models (LLMs) face challenges such as high computational requirements and extensive m...,Large language models (LLMs) are powerful artificial intelligence systems designed to process and ge...,3,0.7331412037213644,29,85
3,What is model compression and why is it important?,"Model compression involves transforming a large, resource-intensive model into a compact version sui...",Model compression refers to the process of reducing the size and complexity of a large language mode...,Model compression refers to the process of reducing the size and complexity of a machine learning mo...,3,0.7489504416783651,68,109
4,What are the main categories of model compression techniques covered in this survey?,"The survey covers four main categories: (1) Quantization, (2) Pruning, (3) Knowledge Distillation, a...","The main categories of model compression techniques covered in this survey are quantization, pruning...",The main categories of model compression techniques covered in this survey are: 1. Quantization 2. P...,3,0.7746127943197886,19,23
5,What metrics are used to evaluate compressed LLMs?,"Key metrics include: Model Size (number of parameters), FLOPs (floating-point operations), MFU (Mean...","The evaluation metrics for compressed LLMs include arithmetic intensity, memory size, and throughput...","Compressed Language Models (LLMs) are evaluated using several metrics, including: 1. Perplexity: Thi...",3,0.7852243880430857,50,39
6,What is the difference between FLOPs and MFU?,FLOPs measures the theoretical computational efficiency (number of floating-point operations require...,Floating Point Operations (FLOPs) and Mean FLOPS Utilization (MFU) are two different metrics used to...,"FLOPs and MFU are both terms used in the context of computer memory management. However, they refer ...",3,0.5975301265716553,83,85
7,What is quantization in the context of LLMs?,"Quantization refers to the process of reducing the number of bits (i.e., precision) in the parameter...","separately. To further improve the quantization performance, ZeroQuant uses a modified version of th...",Quantization refers to the process of reducing the number of levels or categories in a continuous va...,3,0.728348990281423,84,64
8,What is the difference between QAT and PTQ?,The primary distinction lies in whether retraining is needed during quantization. Post-Training Quan...,two main approaches: post-training quantization (PTQ) and training quantization (QA T). PTQ,"QAT stands for Quality Assurance Testing, which is a process that involves testing software or appli...",3,0.6063187420368195,12,117
9,What are the three categories of Post-Training Quantization?,PTQ for LLMs is categorized into three groups: (1) Weight-Only Quantization - focuses solely on quan...,ciencies and reduced computational requirements.,The three main categories of Post-Training Quantization are: 1. Model Compression: This category inv...,3,0.6816064516703287,5,67
10,What is GPTQ and how does it work?,GPTQ is a layer-wise quantization method based on Optimal Brain Quantization (OBQ) that updates weig...,"Pruning, and Channel-wise Pruning. Loss-based pruning relies on the mean squared error or cross-entr...","GPTQ stands for ""Generative Pre-Training Query"". It is a technique used in natural language processi...",3,0.6304977734883627,96,100
11,What is AWQ's approach to weight quantization?,AWQ (Activation-aware Weight Quantization) stores the top 1% of weights that have the most significa...,the performance of quantized LLMs. WKVQuant demonstrates a 20% reduction in memory consumption and a...,AWQ (Adaptive Weight Quantization) is a technique used to reduce the precision of floating-point num...,3,0.7458714346090952,20,111
12,Why do LLMs have issues with activation quantization?,"LLMs have outliers in activations, and the performance declines significantly if these activations w...","separately. However, this method is limited by its dependence on the grouping scheme used to divide ...",Activation quantization is a process used to reduce the precision of the activations in artificial n...,3,0.746030737956365,84,117
13,What is SmoothQuant's approach?,SmoothQuant designs a per-channel scaling transformation to smooth the activation outliers based on ...,SmoothQuant's approach involves using a combination of gradient descent and a momentum term to updat...,SmoothQuant is an open-source quantitative trading framework that provides a set of tools and algori...,3,0.5545153617858887,20,100
14,What is KV cache quantization and why is it important?,"KV cache quantization targets the KV cache, which stores keys and values of attention layers. It's i...","separately. ZeroQuant distills knowledge by generating data from the LLM itself, and train the quant...",Key-Value (KV) cache quantization refers to the process of reducing the precision or bit depth of th...,3,0.714163343111674,93,118
15,What is pruning in neural networks?,Pruning is a powerful technique to reduce the size or complexity of a model by removing redundant co...,"For example, FLAP utilizes a structured fluctuation metric to assess and identify columns in the wei...",Pruning in neural networks refers to the process of removing unnecessary or redundant connections be...,3,0.7517118354638418,104,99
