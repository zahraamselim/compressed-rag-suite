{
  "rag": {
    "document_processing": {
      "remove_headers": true,
      "remove_citations": true,
      "extract_sections": false
    },
    "chunking": {
      "strategy": "semantic",
      "chunk_size": 512,
      "chunk_overlap": 50,
      "min_chunk_size": 100
    },
    "embedding": {
      "model_name": "sentence-transformers/all-MiniLM-L6-v2",
      "device": "cuda",
      "batch_size": 32,
      "normalize": true
    },
    "retrieval": {
      "top_k": 3,
      "similarity_threshold": 0.0,
      "rerank": false,
      "diversity_penalty": 0.0
    },
    "generation": {
      "max_new_tokens": 150,
      "temperature": 0.7,
      "top_p": 0.9,
      "do_sample": true,
      "repetition_penalty": 1.1,
      "use_chat_template": true
    },
    "vector_store": {
      "collection_name": "mistral_7b_fp16",
      "persist_directory": "results/mistral_7b_fp16/vector_store"
    }
  },
  "model": {
    "model_path": "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1",
    "model_type": "instruct",
    "interface_type": "huggingface",
    "quantization": null,
    "torch_dtype": "float16",
    "device_map": "auto",
    "trust_remote_code": false
  },
  "evaluation": {
    "compare_no_rag": true,
    "efficiency": {
      "num_warmup": 3,
      "num_runs": 10,
      "max_new_tokens": 128,
      "prompts": [
        "The capital of France is",
        "Artificial intelligence is defined as",
        "The main benefit of renewable energy sources is",
        "In machine learning, the term 'overfitting' refers to",
        "Quantum computing differs from classical computing because",
        "The theory of relativity states that",
        "Neural networks are computational models inspired by",
        "Climate change is primarily caused by"
      ],
      "measure_prefill_decode": true,
      "measure_batch_throughput": false,
      "batch_sizes": [
        1,
        2,
        4,
        8
      ],
      "estimate_kv_cache": true,
      "baseline_results_path": null
    },
    "performance": {
      "perplexity": {
        "enabled": true,
        "dataset": "wikitext",
        "dataset_config": "wikitext-2-raw-v1",
        "split": "test",
        "num_samples": 100,
        "max_length": 512,
        "stride": null,
        "batch_size": 1
      },
      "lm_eval": {
        "enabled": false,
        "batch_size": 1,
        "tasks": {
          "hellaswag": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - sentence completion"
          },
          "winogrande": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - pronoun resolution"
          },
          "piqa": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - physical interactions"
          },
          "siqa": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - social interactions"
          },
          "openbookqa": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - elementary science"
          },
          "arc_easy": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - science questions (easy)"
          },
          "arc_challenge": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - science questions (hard)"
          },
          "commonsense_qa": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Commonsense reasoning - general knowledge"
          },
          "nq_open": {
            "enabled": false,
            "num_fewshot": 5,
            "limit": null,
            "description": "World knowledge - Natural Questions"
          },
          "triviaqa": {
            "enabled": false,
            "num_fewshot": 5,
            "limit": null,
            "description": "World knowledge - trivia questions"
          },
          "boolq": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Reading comprehension - boolean questions"
          },
          "quac": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Reading comprehension - conversational QA"
          },
          "gsm8k": {
            "enabled": false,
            "num_fewshot": 8,
            "limit": null,
            "description": "Math reasoning - grade school math"
          },
          "hendrycks_math": {
            "enabled": false,
            "num_fewshot": 4,
            "limit": null,
            "description": "Math reasoning - competition mathematics"
          },
          "humaneval": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Code generation - Python (pass@1)"
          },
          "mbpp": {
            "enabled": false,
            "num_fewshot": 3,
            "limit": null,
            "description": "Code generation - Python problems"
          },
          "mmlu": {
            "enabled": false,
            "num_fewshot": 5,
            "limit": null,
            "description": "Aggregate - Massive Multitask Language Understanding"
          },
          "bbh": {
            "enabled": false,
            "num_fewshot": 3,
            "limit": null,
            "description": "Aggregate - BIG-Bench Hard"
          },
          "agieval": {
            "enabled": false,
            "num_fewshot": 3,
            "limit": null,
            "description": "Aggregate - AGI Eval (English only)"
          },
          "lambada": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Language understanding - word prediction"
          },
          "storycloze": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Language understanding - story completion"
          },
          "glue": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "NLP benchmark suite"
          },
          "super_glue": {
            "enabled": false,
            "num_fewshot": 0,
            "limit": null,
            "description": "Advanced NLP benchmark suite"
          }
        }
      }
    },
    "retrieval": {
      "measure_retrieval_quality": true,
      "measure_context_quality": true,
      "measure_answer_quality": true,
      "compare_no_rag": true,
      "k_values": [
        1,
        3,
        5,
        10
      ],
      "relevance_token_threshold": 0.3,
      "sufficiency_token_threshold": 0.8,
      "ndcg_gain_type": "exponential",
      "normalize_whitespace": true,
      "case_sensitive": false,
      "remove_punctuation": false,
      "faithfulness_method": "token_containment",
      "relevance_method": "token_overlap",
      "rouge_use_stemmer": true,
      "bertscore_lang": "en",
      "bertscore_model": null,
      "dataset_path": null,
      "use_custom_dataset": false
    },
    "output_dir": "results/mistral_7b_fp16",
    "save_predictions": true,
    "save_metrics": true
  }
}