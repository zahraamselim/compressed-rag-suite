{
  "model": "Mistral 7B FP16",
  "model_info": {
    "model_path": "/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1",
    "model_type": "instruct",
    "device": "cuda:0",
    "size_gb": 13.488777160644531,
    "num_parameters": 7241732096,
    "gpu_memory_allocated_gb": 6.744384765625,
    "gpu_memory_reserved_gb": 6.74609375
  },
  "config": {
    "num_questions": 15,
    "chunk_size": 512,
    "top_k": 3
  },
  "efficiency": {
    "device": "cuda:0",
    "device_name": "Tesla T4",
    "latency_ms_per_token": 63.62993283440506,
    "latency_std": 0.1311529365278679,
    "ttft_ms": 69.22530920001009,
    "ttft_std": 0.15532323908127427,
    "prefill_ms": 70.66915139998855,
    "decode_ms_per_token": 61.63227613125031,
    "throughput_tokens_per_sec": 15.664722005347258,
    "throughput_std": 0.02318101600717649,
    "peak_memory_mb": 7013.30126953125,
    "model_size_gb": 13.48877739906311,
    "total_params": 7241732096,
    "bits_per_param": 16.0,
    "memory_efficiency": 1.9694730806229597,
    "kv_cache_size_mb": 1024.0,
    "flops_per_token_gflops": 12.073828352,
    "mfu_percent": 2.3349773416587656,
    "energy_per_token_mj": 4454.095298408353,
    "tdp_watts": 70.0,
    "compression_ratio": null,
    "speedup": null,
    "memory_reduction": null,
    "batch_throughput": null
  },
  "performance": {
    "perplexity": 13.313827352511062,
    "perplexity_dataset": "wikitext/wikitext-2-raw-v1",
    "lm_eval_scores": {},
    "average_accuracy": null,
    "num_tasks_evaluated": 0
  },
  "retrieval": {
    "precision_at_1": null,
    "precision_at_3": null,
    "precision_at_5": null,
    "precision_at_10": null,
    "recall_at_1": null,
    "recall_at_3": null,
    "recall_at_5": null,
    "recall_at_10": null,
    "f1_at_1": null,
    "f1_at_3": null,
    "f1_at_5": null,
    "f1_at_10": null,
    "mrr": null,
    "map_score": null,
    "context_sufficiency": 0.7859820244670939,
    "context_precision": 0.5912901912901912,
    "context_coverage": 0.7378797738911591,
    "avg_context_length": 1267.8666666666666,
    "avg_retrieval_score": 0.6917134172386593,
    "retrieval_consistency": 0.07784678330061884,
    "avg_chunks_retrieved": 3.0,
    "exact_match": 0.0,
    "f1_score": 0.22663061940609988,
    "answer_relevance": 0.10618511294783774,
    "faithfulness": 0.7082919395843932,
    "rouge1": 0.2886013927664643,
    "rouge2": 0.0844720124919429,
    "rougeL": 0.2235347195851685,
    "bertscore_f1": 0.8628257513046265,
    "avg_answer_length": 58.06666666666667,
    "no_rag_f1": 0.17674115360814385,
    "no_rag_exact_match": 0.0,
    "f1_improvement": 0.04988946579795603,
    "em_improvement": 0.0,
    "evaluation_mode": "qa",
    "num_questions": 15
  },
  "timestamp": "2025-11-02 08:03:49"
}